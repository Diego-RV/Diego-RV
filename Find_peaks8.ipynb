{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e186e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pastaq as pq\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc9f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [{'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE20_exc0-p1_2.ms2\"},\n",
    "               { 'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE20_exc2-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE20_exc10-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE30_exc0-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE30_exc2-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE30_exc10-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE40_exc0-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE40_exc2-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE40_exc10-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE50_exc0-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE50_exc2-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE50_exc10-p1_2.ms2\"},\n",
    "                {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE60_exc0-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE60_exc2-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE60_exc10-p1_2.ms2\"},\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69615fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_clusters_annotations_csv = pd.read_csv(r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\quant\\feature_clusters_annotations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a6ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9654b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import ms_entropy as me\n",
    "\n",
    "def combine_multiple_samples(feature_clusters_annotations_csv, input_files, output_dir):\n",
    "    # Preprocess: build a lookup dictionary to avoid filtering the DataFrame each time\n",
    "    annotations_lookup = defaultdict(list)\n",
    "\n",
    "    for _, row in feature_clusters_annotations_csv.iterrows():\n",
    "        if pd.notnull(row['msms_id']):\n",
    "            key = (row['file_id'], row['msms_id'])\n",
    "            annotations_lookup[key].append(row)\n",
    "\n",
    "    combined_multiple_samples = []\n",
    "\n",
    "    for file in input_files:\n",
    "        if 'stem' not in file:\n",
    "            base_name = os.path.splitext(os.path.basename(file['raw_path']))[0]\n",
    "            file['stem'] = base_name\n",
    "        stem = file['stem']\n",
    "        in_path = os.path.join(output_dir, 'raw', f\"{stem}.ms2\")\n",
    "\n",
    "        if not os.path.exists(in_path):\n",
    "            continue\n",
    "\n",
    "        raw_data = pq.read_raw_data(in_path)\n",
    "\n",
    "        for scan in raw_data.scans:\n",
    "            scan_number = scan.scan_number\n",
    "            key = (stem, scan_number)\n",
    "            annotations = annotations_lookup.get(key)\n",
    "\n",
    "            if not annotations:\n",
    "                continue\n",
    "\n",
    "            ms2_mz = scan.mz\n",
    "            ms2_intensity = scan.intensity\n",
    "            ms2_rt = scan.retention_time\n",
    "\n",
    "            if not ms2_mz or not ms2_intensity or len(ms2_mz) != len(ms2_intensity):\n",
    "                continue\n",
    "\n",
    "            # Convert to numpy array for faster sorting\n",
    "            mz_array = np.array(ms2_mz)\n",
    "            intensity_array = np.array(ms2_intensity)\n",
    "            sorted_indices = np.argsort(mz_array)\n",
    "            mz_intensity_pairs = list(zip(mz_array[sorted_indices], intensity_array[sorted_indices]))\n",
    "            ms2_peaks = np.array(mz_intensity_pairs, dtype=np.float32)\n",
    "            centroided_peaks = me.clean_spectrum(ms2_peaks, min_ms2_difference_in_da = 0.02, normalize_intensity = False)\n",
    "            # Extract centroided m/z and intensity values from the sample\n",
    "            cent_mz, cent_int = zip(*centroided_peaks)\n",
    "\n",
    "            for row in annotations:\n",
    "                combined_multiple_samples.append({\n",
    "                    'cluster_id': row['cluster_id'],\n",
    "                    'file_id': row['file_id'],\n",
    "                    'feature_id': row['feature_id'],\n",
    "                    'peak_id': row['peak_id'],\n",
    "                    'msms_id': row['msms_id'],\n",
    "                    'ms2_rt': ms2_rt,\n",
    "                    'charge_state': row['charge_state'],\n",
    "                    'cent_mz' : cent_mz,\n",
    "                    'cent_int' : cent_int,\n",
    "                    'ms2_peaks' : ms2_peaks\n",
    "                })\n",
    "\n",
    "\n",
    "    return combined_multiple_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637e8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_multiple_samples = combine_multiple_samples(feature_clusters_annotations_csv, input_files, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb49be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_multiple_samples = combine_multiple_samples(feature_clusters_annotations_csv, input_files, output_dir)\n",
    "from pathlib import Path  \n",
    "filepath = Path('CID_metadata/CID_CORRECT/combined_multiple_samples.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "combined_multiple_samples_df = pd.DataFrame(combined_multiple_samples)\n",
    "combined_multiple_samples_df.to_csv(Path('CID_metadata/CID_CORRECT/combined_multiple_samples.csv', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aead587",
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_file = r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\Msp_CID_2025_03_21_12_30_01_AlignmentResult_2025_03_19_11_06_05.msp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import ms_entropy as me\n",
    "\n",
    "def link_features(combined_multiple_samples_csv, input_files, output_dir):\n",
    "    # Preprocess: build a lookup dictionary to avoid filtering the DataFrame each time\n",
    "    annotations_lookup = defaultdict(list)\n",
    "\n",
    "    for _, row in combined_multiple_samples_csv.iterrows():\n",
    "        if pd.notnull(row['feature_id']):\n",
    "            key = (row['file_id'], row['feature_id'])\n",
    "            annotations_lookup[key].append(row)\n",
    "\n",
    "    linked_features = []\n",
    "\n",
    "    for file in input_files:\n",
    "        if 'stem' not in file:\n",
    "            base_name = os.path.splitext(os.path.basename(file['raw_path']))[0]\n",
    "            file['stem'] = base_name\n",
    "        stem = file['stem']\n",
    "        in_path_features = os.path.join(output_dir, 'features', f\"{stem}.features\")\n",
    "\n",
    "        if not os.path.exists(in_path_features):\n",
    "            continue\n",
    "\n",
    "        features = pq.read_features(in_path_features)\n",
    "\n",
    "        for feature in features:\n",
    "            id = feature.id\n",
    "            key = (stem, id)\n",
    "            annotations = annotations_lookup.get(key)\n",
    "\n",
    "            if not annotations:\n",
    "                continue\n",
    "\n",
    "            features_df = pd.DataFrame({\n",
    "                'feature_id': [feature.id for feature in features],\n",
    "                'average_mz': [feature.average_mz for feature in features],\n",
    "                'average_mz_sigma': [feature.average_mz_sigma for feature in features],\n",
    "                'average_rt': [feature.average_rt for feature in features],\n",
    "                'average_rt_sigma': [feature.average_rt_sigma for feature in features],\n",
    "                'average_rt_delta': [feature.average_rt_delta for feature in features],\n",
    "                'total_height': [feature.total_height for feature in features],\n",
    "                'total_volume': [feature.total_volume for feature in features],\n",
    "                'monoisotopic_mz': [feature.monoisotopic_mz for feature in features],\n",
    "                'monoisotopic_rt' : [feature.monoisotopic_rt for feature in features],\n",
    "                'monoisotopic_height': [feature.monoisotopic_height for feature in features],\n",
    "                'monoisotopic_volume': [feature.monoisotopic_volume for feature in features],\n",
    "                'charge_state': [feature.charge_state for feature in features],\n",
    "                'peak_id': [feature.peak_ids for feature in features],\n",
    "            })\n",
    "\n",
    "            precursor_intensity =feature.monoisotopic_height\n",
    "            precursor_rt = feature.monoisotopic_rt\n",
    "            precursor_mz = feature.monoistotopic_mz\n",
    "            precursor_vol = feature.monoisotopic_volume\n",
    "            average_ms1_mz = feature.average_mz\n",
    "            average_rt =feature.average_rt\n",
    "            charge_state = feature.charge_state\n",
    "\n",
    "\n",
    "            if not ms2_mz or not ms2_intensity or len(ms2_mz) != len(ms2_intensity):\n",
    "                continue\n",
    "\n",
    "            # Convert to numpy array for faster sorting\n",
    "            mz_array = np.array(ms2_mz)\n",
    "            intensity_array = np.array(ms2_intensity)\n",
    "            sorted_indices = np.argsort(mz_array)\n",
    "            mz_intensity_pairs = list(zip(mz_array[sorted_indices], intensity_array[sorted_indices]))\n",
    "            ms2_peaks = np.array(mz_intensity_pairs, dtype=np.float32)\n",
    "            centroided_peaks = me.clean_spectrum(ms2_peaks, min_ms2_difference_in_da = 0.02, normalize_intensity = False)\n",
    "            # Extract centroided m/z and intensity values from the sample\n",
    "            cent_mz, cent_int = zip(*centroided_peaks)\n",
    "\n",
    "            for row in annotations:\n",
    "                combined_multiple_samples.append({\n",
    "                    'cluster_id': row['cluster_id'],\n",
    "                    'file_id': row['file_id'],\n",
    "                    'feature_id': row['feature_id'],\n",
    "                    'peak_id': row['peak_id'],\n",
    "                    'msms_id': row['msms_id'],\n",
    "                    'ms2_rt': ms2_rt,\n",
    "                    'charge_state': row['charge_state'],\n",
    "                    'cent_mz' : cent_mz,\n",
    "                    'cent_int' : cent_int,\n",
    "                    'ms2_peaks' : ms2_peaks\n",
    "                })\n",
    "\n",
    "\n",
    "    return combined_multiple_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85307a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "            precursor_intensity =feature.monoisotopic_height\n",
    "            precursor_rt = feature.monoisotopic_rt\n",
    "            precursor_mz = feature.monoistotopic_mz\n",
    "            precursor_vol = feature.monoisotopic_volume\n",
    "            average_ms1_mz = feature.average_mz\n",
    "            average_rt =feature.average_rt\n",
    "            charge_state = feature.charge_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "504bfec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [{'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE20_exc0-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE20_exc2-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE20_exc10-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE30_exc0-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE30_exc2-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE30_exc10-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE40_exc0-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE40_exc2-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE40_exc10-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE50_exc0-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE50_exc2-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE50_exc10-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE60_exc0-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE60_exc2-p1_2.features\"},\n",
    "                {'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\features\\p_CE60_exc10-p1_2.features\"}\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d1a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import ms_entropy as me\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def link_msp(\n",
    "    msp_file, input_files, output_dir, combined_multiple_samples,\n",
    "    mz_tolerance=0.025, rt_tolerance=8.0\n",
    "):\n",
    "    annotations_lookup = defaultdict(list)\n",
    "    linked_msp = []\n",
    "\n",
    "    for ann in combined_multiple_samples:\n",
    "        if pd.notnull(ann['feature_id']):\n",
    "            key = (ann['file_id'], ann['feature_id'])\n",
    "            annotations_lookup[key].append(ann)\n",
    "\n",
    "    msp_data = pq.read_msp_file(msp_file)\n",
    "\n",
    "    for file in input_files:\n",
    "        if 'stem' not in file:\n",
    "            file['stem'] = os.path.splitext(os.path.basename(file['raw_path']))[0]\n",
    "        stem = file['stem']\n",
    "        in_path_features = os.path.join(output_dir, 'features', f\"{stem}.features\")\n",
    "\n",
    "        if not os.path.exists(in_path_features):\n",
    "            print(f\"Warning: Missing features file: {in_path_features}\")\n",
    "            continue\n",
    "\n",
    "        features = pq.read_features(in_path_features)\n",
    "\n",
    "        for feature in features:\n",
    "            id = feature.id\n",
    "            key = (stem, id)\n",
    "            annotations = annotations_lookup.get(key)\n",
    "\n",
    "            if not annotations:\n",
    "                continue\n",
    "\n",
    "            for annotation in annotations:\n",
    "                file_id = annotation['file_id']\n",
    "                cluster_id = annotation['cluster_id']\n",
    "                msms_id = annotation['msms_id']\n",
    "                ms2_sample_peaks = annotation['ms2_peaks']\n",
    "\n",
    "                msp_data = pq.read_msp_file(msp_file)\n",
    "\n",
    "                # Centroid the MS2 spectrum\n",
    "                centroided_peaks = me.clean_spectrum(\n",
    "                    ms2_sample_peaks,\n",
    "                    min_ms2_difference_in_da=0.02,\n",
    "                    normalize_intensity=False\n",
    "                )\n",
    "\n",
    "                centroided_arr = np.array(centroided_peaks)\n",
    "                centroided_arr_list = centroided_arr.tolist()\n",
    "\n",
    "                sample_mz, sample_intensity = zip(*centroided_peaks)\n",
    "                sample_mz = np.array(sample_mz)\n",
    "                sample_intensity = np.array(sample_intensity, dtype=np.float32)\n",
    "\n",
    "                precursor_mz = feature.monoisotopic_mz\n",
    "                precursor_rt = feature.monoisotopic_rt\n",
    "\n",
    "                best_match = None\n",
    "\n",
    "                for data in msp_data:\n",
    "                    if 'precursor_mz' not in data or 'retention_time' not in data:\n",
    "                        continue\n",
    "\n",
    "                    mz_dist = np.abs(precursor_mz - data['precursor_mz'])\n",
    "                    rt_dist = np.abs(precursor_rt - data['retention_time'])\n",
    "\n",
    "                    if mz_dist <= mz_tolerance and rt_dist <= rt_tolerance:\n",
    "                        score = (mz_dist * 20) + (rt_dist * 0.0625)\n",
    "                        mass_error_ppm = (\n",
    "                            (data['precursor_mz'] - precursor_mz) / data['precursor_mz']\n",
    "                        ) * 1e6\n",
    "                        y_true_mz = [data.get('precursor_mz')]\n",
    "                        y_pred_mz = [precursor_mz]\n",
    "                        rmse_mz = root_mean_squared_error(y_true_mz, y_pred_mz)\n",
    "\n",
    "                        reference_peaks = data.get('peaks', [])\n",
    "                        ref_mz, ref_intensity = zip(*reference_peaks)\n",
    "                        ref_mz = np.array(ref_mz)\n",
    "                        ref_intensity = np.array(ref_intensity, dtype=np.float32)\n",
    "\n",
    "                        # Dot Product\n",
    "                        matched_idx = np.searchsorted(ref_mz, sample_mz)\n",
    "                        matched_idx = matched_idx[matched_idx < len(ref_mz)]\n",
    "\n",
    "                        sample_spectrum = sample_intensity[:len(matched_idx)]\n",
    "                        reference_spectrum = ref_intensity[matched_idx]\n",
    "\n",
    "                        sample_norm = sample_spectrum / np.linalg.norm(sample_spectrum)\n",
    "                        reference_norm = reference_spectrum / np.linalg.norm(reference_spectrum)\n",
    "                        if len(centroided_peaks) < 3 or len(reference_peaks) < 3:\n",
    "                            dot_product = None\n",
    "                        else:\n",
    "                            dot_product = np.dot(sample_norm, reference_norm)\n",
    "\n",
    "                        similarity = me.calculate_entropy_similarity(\n",
    "                            np.array(ms2_sample_peaks, dtype=np.float32),\n",
    "                            np.array(reference_peaks, dtype=np.float32)\n",
    "                        )\n",
    "                        unweighted_similarity = me.calculate_unweighted_entropy_similarity(\n",
    "                            np.array(ms2_sample_peaks, dtype=np.float32),\n",
    "                            np.array(reference_peaks, dtype=np.float32)\n",
    "                        )\n",
    "\n",
    "                        if best_match is None or score < best_match['score']:\n",
    "                            best_match = {\n",
    "                                'score': float(score),\n",
    "                                'mass_error_ppm': float(mass_error_ppm),\n",
    "                                'name': data.get('name'),\n",
    "                                'retention_time': data.get('retention_time'),\n",
    "                                'precursor_mz': data.get('precursor_mz'),\n",
    "                                'precursor_type': data.get('precursor_type'),\n",
    "                                'smiles': data.get('smiles'),\n",
    "                                'saturation': data.get('saturation'),\n",
    "                                'msp_peaks': reference_peaks\n",
    "                            }\n",
    "\n",
    "                            annotated_feature = {\n",
    "                                'cluster_id': cluster_id,\n",
    "                                'file_id' : file_id,\n",
    "                                'feature_id': id,\n",
    "                                'msms_id': msms_id,\n",
    "                                'peak_ids': feature.peak_ids,\n",
    "                                'dot_product': float(dot_product) if dot_product is not None else 'NA',\n",
    "                                'similarity': float(similarity),\n",
    "                                'unweighted_similarity': float(unweighted_similarity),\n",
    "                                'centroided_peaks': centroided_arr_list,\n",
    "                                'rmse_mz': float(rmse_mz),\n",
    "                                'mass_error_ppm': mass_error_ppm,\n",
    "                                'precursor_intensity': feature.monoisotopic_height,\n",
    "                                'precursor_rt': precursor_rt,\n",
    "                                'precursor_vol': feature.monoisotopic_volume,\n",
    "                                'average_ms1_mz': feature.average_mz,\n",
    "                                'average_rt': feature.average_rt,\n",
    "                                'charge_state': feature.charge_state,\n",
    "                                'matches': [best_match]\n",
    "                            }\n",
    "\n",
    "                if best_match:\n",
    "                    linked_msp.append(annotated_feature)\n",
    "\n",
    "    return linked_msp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10fafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import pickle  # for shared data serialization\n",
    "import pastaq as pq  \n",
    "\n",
    "def build_annotations_lookup(combined_multiple_samples):\n",
    "    lookup = defaultdict(list)\n",
    "    for ann in combined_multiple_samples:\n",
    "        if pd.notnull(ann['feature_id']):\n",
    "            key = (ann['file_id'], ann['feature_id'])\n",
    "            lookup[key].append(ann)\n",
    "    return lookup\n",
    "\n",
    "# This function processes ONE file\n",
    "def process_file(file, output_dir, annotations_lookup):\n",
    "    results = []\n",
    "\n",
    "    if 'stem' not in file:\n",
    "        file['stem'] = os.path.splitext(os.path.basename(file['raw_path']))[0]\n",
    "    stem = file['stem']\n",
    "    in_path_features = os.path.join(output_dir, 'features', f\"{stem}.features\")\n",
    "\n",
    "    if not os.path.exists(in_path_features):\n",
    "        print(f\"Warning: Missing features file: {in_path_features}\")\n",
    "        return [] #returns empty list skipping file\n",
    "\n",
    "    features = pq.read_features(in_path_features)\n",
    "\n",
    "    for feature in features:\n",
    "        id = feature.id\n",
    "        key = (stem, id)\n",
    "        annotations = annotations_lookup.get(key)\n",
    "\n",
    "        if not annotations:\n",
    "            continue\n",
    "\n",
    "        for annotation in annotations:\n",
    "            results.append({\n",
    "                'cluster_id': annotation['cluster_id'],\n",
    "                'file_id': annotation['file_id'],\n",
    "                'feature_id': id,\n",
    "                'msms_id': annotation['msms_id'],\n",
    "                'peak_ids': feature.peak_ids,\n",
    "                'precursor_intensity': feature.monoisotopic_height,\n",
    "                'precursor_rt': feature.monoisotopic_rt,\n",
    "                'precursor_vol': feature.monoisotopic_volume,\n",
    "                'average_ms1_mz': feature.average_mz,\n",
    "                'average_rt': feature.average_rt,\n",
    "                'charge_state': feature.charge_state,\n",
    "                'ms2_sample_peaks': annotation['ms2_peaks']\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_features(input_files, output_dir, combined_multiple_samples):\n",
    "    annotations_lookup = build_annotations_lookup(combined_multiple_samples)\n",
    "\n",
    "    # Optional: If annotations_lookup is large, use a shared read-only copy\n",
    "    # using pickle to avoid memory overhead\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_file, file, output_dir, annotations_lookup)\n",
    "            for file in input_files\n",
    "        ] #run tasks in parallel\n",
    "\n",
    "        linked_features = []\n",
    "        for future in futures:\n",
    "            linked_features.extend(future.result())\n",
    "\n",
    "    return linked_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc383da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_features = link_features(input_files, output_dir, combined_multiple_samples=combined_multiple_samples)\n",
    "from pathlib import Path  \n",
    "filepath = Path('CID_metadata/CID_CORRECT/linked_features.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "linked_features_df = pd.DataFrame(linked_features)\n",
    "linked_features_df.to_csv(Path('CID_metadata/CID_CORRECT/linked_features.csv', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "from scipy.spatial import KDTree\n",
    "import ms_entropy as me\n",
    "\n",
    "# Update -> working (~7.5 min)\n",
    "# function to annotate the features csv file output from running the DDA pipeline with msp annotations:\n",
    "def link_msp(linked_features_csv, msp_data, mz_tolerance=0.025, rt_tolerance=8.0): # Changed to be the same as default in MSdial for MS2 -> maybe set to be customizable parameter -> changed rt_tol to 8\n",
    "    linked_msp = []\n",
    "\n",
    "    # Loop through a list of Feature objects\n",
    "    for row in linked_features_csv.itertuples(index=False):\n",
    "        average_mz = row.average_mz  # using 'average_mz' from features as mz (is actually weighted average)\n",
    "        total_intensity = row.total_height  # using 'total_height' from features\n",
    "        average_rt = row.average_rt  # average retention time from features (in seconds)\n",
    "        total_volume = row.total_volume # total volume of the feature\n",
    "        precursor_mz = row.monoisotopic_mz  # Using monoisotopic mz from features as precursor mz (fitted mz from peak)\n",
    "        precursor_intensity = row.monoisotopic_height # intensity of the precursor (fitted height from peak)\n",
    "        precursor_volume = row.monoisotopic_volume # volume of the precursor (peaks?)\n",
    "        precursor_rt = row.monoisotopic_rt #retention time (in seconds) of the precursor\n",
    "        charge_state = row.charge_state #charge state\n",
    "        feature_id = row.feature_id\n",
    "        feature_peak_ids = row.peak_ids # the ids of the peaks generated by the pastaq peaks object\n",
    "        cluster_id = row.cluster_id\n",
    "        file_id =  row.file_id\n",
    "        ms2_sample_peaks = row.ms2_sample_peaks\n",
    "        normalized_area = precursor_volume * 114.7977026\n",
    "\n",
    "    # Load and filter MSP annotations once\n",
    "    msp_data = pq.read_msp_file(msp_file)\n",
    "    filtered_msp = [\n",
    "        ann for ann in msp_data\n",
    "        if 'peaks' in ann and 'precursor_mz' in ann and 'retention_time' in ann]\n",
    "    lib_mzs = np.array([ann['precursor_mz'] for ann in filtered_msp])\n",
    "    tree = KDTree(lib_mzs.reshape(-1, 1))\n",
    "    kd_tree = tree.query_ball_point([[precursor_mz]], r=mz_tolerance)[0]\n",
    "    candidates = [filtered_msp[i] for i in kd_tree]\n",
    "    for candidate in candidates:\n",
    "        # Calculate mz and rt distances directly for scalars\n",
    "        mz_distance = np.abs(precursor_mz - ann['precursor_mz']) # changed this from 'mz' to 'precursor_mz'\n",
    "        rt_distance = np.abs(precursor_rt - ann['retention_time'])  # changed this from 'retention_time' to 'precursor_rt' \n",
    "\n",
    "        # Apply the tolerance checks\n",
    "        if mz_distance <= mz_tolerance and rt_distance <= rt_tolerance:\n",
    "            # If the distances are within tolerance, calculate the match score with normalization factor\n",
    "            match_score = (rt_distance*0.025) + (mz_distance*20) # lower score is better/ closer match; added weighting\n",
    "                \n",
    "                # Calculate mass error in ppm using formula\n",
    "                mass_error_ppm = ((annotation['precursor_mz'] - precursor_mz) / annotation['precursor_mz']) * 10**6\n",
    "                \n",
    "                #Calculate root mean squared error for best match\n",
    "                y_true_mz = [annotation.get('precursor_mz')]\n",
    "                y_pred_mz = [precursor_mz]\n",
    "                rmse_mz = root_mean_squared_error(y_true_mz, y_pred_mz)\n",
    "                \n",
    "                if best_match is None or match_score < best_match['score']:\n",
    "                    best_match = {\n",
    "                        'score': match_score,  # Store the match score\n",
    "                        'mass_error_ppm' : mass_error_ppm,\n",
    "                        'rmse_mz' : rmse_mz,\n",
    "                        'name': annotation.get('name', None),\n",
    "                        'saturation' : annotation.get('saturation', None),\n",
    "                        'retention_time': annotation.get('retention_time', None),\n",
    "                        'precursor_mz': annotation.get('precursor_mz', None),\n",
    "                        'precursor_type': annotation.get('precursor_type', None),\n",
    "                        'smiles': annotation.get('smiles', None),\n",
    "                        'msp_peaks': annotation.get('peaks', None),\n",
    "                    }\n",
    "        \n",
    "        # Create the annotated scan with only the best match\n",
    "        annotated_feature_csv = {\n",
    "            'average_mz': average_mz,\n",
    "            'total_intensity': total_intensity,  # Intensity values\n",
    "            'average_rt': average_rt,  # Average retention time (in seconds)\n",
    "            'total_volume' : total_volume,\n",
    "            'precursor_mz': precursor_mz,\n",
    "            'precursor_intensity' : precursor_intensity,\n",
    "            'precursor_rt' : precursor_rt,\n",
    "            'precursor_volume' : precursor_volume,\n",
    "            'normalized_area' : normalized_area,\n",
    "            'charge_state' : charge_state,\n",
    "            'feature_id': feature_id,\n",
    "            'feature_peak_ids' : feature_peak_ids,\n",
    "            'matches': []  # List to hold the top match\n",
    "        }\n",
    "\n",
    "        # Add the best match if available\n",
    "        if best_match:\n",
    "            annotated_feature_csv['matches'].append(best_match)\n",
    "        else:\n",
    "            annotated_feature_csv['matches'] = None  # No match found\n",
    "        \n",
    "        annotated_features_csv.append(annotated_feature_csv)\n",
    "\n",
    "    # Return after all features are processed\n",
    "    return annotated_features_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def link_features(input_files, output_dir, combined_multiple_samples):\n",
    "    annotations_lookup = defaultdict(list)\n",
    "\n",
    "    for file in input_files:\n",
    "        if 'stem' not in file:\n",
    "            file['stem'] = os.path.splitext(os.path.basename(file['raw_path']))[0]\n",
    "            stem = file['stem']\n",
    "            in_path_features = os.path.join(output_dir, 'features', f\"{stem}.features\")\n",
    "\n",
    "            \n",
    "        if not os.path.exists(in_path_features):\n",
    "            print(f\"Warning: Missing features file: {in_path_features}\")\n",
    "            continue\n",
    "\n",
    "    features = pq.read_features(in_path_features)\n",
    "\n",
    "    features_df = pd.DataFrame({\n",
    "        'feature_id': [feature.id for feature in features],\n",
    "        'average_mz': [feature.average_mz for feature in features],\n",
    "        'average_mz_sigma': [feature.average_mz_sigma for feature in features],\n",
    "        'average_rt': [feature.average_rt for feature in features],\n",
    "        'average_rt_sigma': [feature.average_rt_sigma for feature in features],\n",
    "        'average_rt_delta': [feature.average_rt_delta for feature in features],\n",
    "        'total_height': [feature.total_height for feature in features],\n",
    "        'total_volume': [feature.total_volume for feature in features],\n",
    "        'monoisotopic_mz': [feature.monoisotopic_mz for feature in features],\n",
    "        'monoisotopic_rt' : [feature.monoisotopic_rt for feature in features],\n",
    "        'monoisotopic_height': [feature.monoisotopic_height for feature in features],\n",
    "        'monoisotopic_volume': [feature.monoisotopic_volume for feature in features],\n",
    "        'charge_state': [feature.charge_state for feature in features],\n",
    "        'peak_id': [feature.peak_ids for feature in features],\n",
    "    })\n",
    "    key = (stem, id)\n",
    "    annotations = annotations_lookup.get(key)\n",
    "        if not annotations:\n",
    "            continue\n",
    "\n",
    "\n",
    "    for ann in combined_multiple_samples:\n",
    "        if pd.notnull(ann['feature_id']):\n",
    "            key = (ann['file_id'], ann['feature_id'])\n",
    "            annotations_lookup[key].append(ann)\n",
    "\n",
    "        linked_features = []\n",
    "\n",
    "        features = pq.read_features(in_path_features)\n",
    "        for feature in features:\n",
    "            id = feature.id\n",
    "            key = (stem, id)\n",
    "            annotations = annotations_lookup.get(key)\n",
    "\n",
    "\n",
    "\n",
    "            for annotation in annotations:\n",
    "                file_id = annotation['file_id']\n",
    "                cluster_id = annotation['cluster_id']\n",
    "                msms_id = annotation['msms_id']\n",
    "                ms2_sample_peaks = annotation['ms2_peaks']\n",
    "\n",
    "                \n",
    "                linked_features.append ({\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'file_id' : file_id,\n",
    "                    'feature_id': id,\n",
    "                    'msms_id': msms_id,\n",
    "                    'peak_ids': feature.peak_ids,\n",
    "                    'precursor_intensity': feature.monoisotopic_height,\n",
    "                    'precursor_rt': feature.monoisotopic_rt,\n",
    "                    'precursor_vol': feature.monoisotopic_volume,\n",
    "                    'average_ms1_mz': feature.average_mz,\n",
    "                    'average_rt': feature.average_rt,\n",
    "                    'charge_state': feature.charge_state,\n",
    "                    'ms2_sample_peaks' : ms2_sample_peaks\n",
    "                     })\n",
    "\n",
    "    return linked_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20b89312",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m linked_features \u001b[38;5;241m=\u001b[39m \u001b[43mlink_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_multiple_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_multiple_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path  \n\u001b[0;32m      3\u001b[0m filepath \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCID_metadata/CID_CORRECT/linked_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n",
      "Cell \u001b[1;32mIn[9], line 31\u001b[0m, in \u001b[0;36mlink_features\u001b[1;34m(input_files, output_dir, combined_multiple_samples)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m---> 31\u001b[0m     key \u001b[38;5;241m=\u001b[39m (stem, \u001b[38;5;28mid\u001b[39m)\n\u001b[0;32m     32\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m annotations_lookup\u001b[38;5;241m.\u001b[39mget(key)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m annotations:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "linked_features = link_features(input_files, output_dir, combined_multiple_samples=combined_multiple_samples)\n",
    "from pathlib import Path  \n",
    "filepath = Path('CID_metadata/CID_CORRECT/linked_features.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "linked_features_df = pd.DataFrame(linked_features)\n",
    "linked_features_df.to_csv(Path('CID_metadata/CID_CORRECT/linked_features.csv', index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "            features = pastaq.read_features(in_path_features)\n",
    "\n",
    "            _custom_log(\"Generating features quantitative table\", logger)\n",
    "            features_df = pd.DataFrame({\n",
    "                'feature_id': [feature.id for feature in features],\n",
    "                'average_mz': [feature.average_mz for feature in features],\n",
    "                'average_mz_sigma': [feature.average_mz_sigma for feature in features],\n",
    "                'average_rt': [feature.average_rt for feature in features],\n",
    "                'average_rt_sigma': [feature.average_rt_sigma for feature in features],\n",
    "                'average_rt_delta': [feature.average_rt_delta for feature in features],\n",
    "                'total_height': [feature.total_height for feature in features],\n",
    "                'total_volume': [feature.total_volume for feature in features],\n",
    "                'monoisotopic_mz': [feature.monoisotopic_mz for feature in features],\n",
    "                'monoisotopic_rt' : [feature.monoisotopic_rt for feature in features],\n",
    "                'monoisotopic_height': [feature.monoisotopic_height for feature in features],\n",
    "                'monoisotopic_volume': [feature.monoisotopic_volume for feature in features],\n",
    "                'charge_state': [feature.charge_state for feature in features],\n",
    "                'peak_id': [feature.peak_ids for feature in features],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4120b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "# Load and filter MSP annotations once\n",
    "msp_entries = [ann for ann in pq.read_msp_file(msp_file)\n",
    "               if 'peaks' in ann and 'precursor_mz' in ann and 'retention_time' in ann]\n",
    "precursor_array = np.array([ann['precursor_mz'] for ann in msp_entries]).reshape(-1,1)\n",
    "\n",
    "# Build the tree (fast lookups O(logâ€¯M)): :contentReference[oaicite:2]{index=2}\n",
    "tree = KDTree(precursor_array)\n",
    "\n",
    "# Reshape the query point to 2D array for KDTree\n",
    "query_point = np.array([[precursor_mz]])\n",
    "idxs = tree.query_ball_point(query_point, r=mz_tolerance)[0]  # all indices within tol\n",
    "\n",
    "candidates = [msp_entries[i] for i in idxs]\n",
    "# Now only run detailed matching (RT, entropy, etc.) on these candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the query point to 2D array for KDTree\n",
    "query_point = np.array([[precursor_mz]])\n",
    "idxs = tree.query_ball_point(query_point, r=mz_tolerance)[0]  # all indices within tol\n",
    "\n",
    "candidates = [msp_entries[i] for i in idxs]\n",
    "# Now only run detailed matching (RT, entropy, etc.) on these candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare tree\n",
    "msp_entries = [...]\n",
    "precursor_array = np.array([...]).reshape(-1,1)\n",
    "tree = KDTree(precursor_array)\n",
    "\n",
    "# 2. Inside your loop, for each peak:\n",
    "query = np.array([[ms1_mz]])\n",
    "idxs = tree.query_ball_point(query, r=mz_tolerance)[0]\n",
    "for idx in idxs:\n",
    "    ann = msp_entries[idx]\n",
    "    # Check rt tolerance, compute scores...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c39a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_msp = link_msp(msp_file=msp_file, output_dir=output_dir, input_files=input_files, combined_multiple_samples=combined_multiple_samples, mz_tolerance=0.025, rt_tolerance=8.0)\n",
    "from pathlib import Path  \n",
    "filepath = Path('CID_metadata/CID_CORRECT/linked_msp_matches.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "linked_msp_df = pd.DataFrame(linked_msp)\n",
    "linked_msp_df.to_csv(Path('CID_metadata/CID_CORRECT/linked_msp_matches.csv', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b47bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "                            # Calculate geometric mean weights\n",
    "                            weights = np.sqrt(sample_norm * reference_norm)\n",
    "\n",
    "                            # Compute weighted dot product\n",
    "                            weighted_dot_product = np.sum(weights * sample_norm * reference_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "# from concurrent.futures import ProcessPoolExecutor  # Uncomment for parallelism\n",
    "\n",
    "def combine_multiple_samples(feature_clusters_annotations_csv, input_files, output_dir):\n",
    "    # Preprocess: build a lookup dictionary to avoid filtering the DataFrame each time\n",
    "    annotations_lookup = defaultdict(list)\n",
    "\n",
    "    for _, row in feature_clusters_annotations_csv.iterrows():\n",
    "        if pd.notnull(row['msms_id']):\n",
    "            key = (row['file_id'], row['msms_id'])\n",
    "            annotations_lookup[key].append(row)\n",
    "\n",
    "    combined_multiple_samples = []\n",
    "\n",
    "    for file in input_files:\n",
    "        if 'stem' not in file:\n",
    "            base_name = os.path.splitext(os.path.basename(file['raw_path']))[0]\n",
    "            file['stem'] = base_name\n",
    "        stem = file['stem']\n",
    "        in_path = os.path.join(output_dir, 'raw', f\"{stem}.ms2\")\n",
    "\n",
    "        if not os.path.exists(in_path):\n",
    "            continue\n",
    "\n",
    "        raw_data = pq.read_raw_data(in_path)\n",
    "\n",
    "        for scan in raw_data.scans:\n",
    "            scan_number = scan.scan_number\n",
    "            key = (stem, scan_number)\n",
    "            annotations = annotations_lookup.get(key)\n",
    "\n",
    "            if not annotations:\n",
    "                continue\n",
    "\n",
    "            ms2_mz = scan.mz\n",
    "            ms2_intensity = scan.intensity\n",
    "            ms2_rt = scan.retention_time\n",
    "            ms2_precursor_info = scan.precursor_information\n",
    "\n",
    "            if not ms2_mz or not ms2_intensity or len(ms2_mz) != len(ms2_intensity):\n",
    "                continue\n",
    "\n",
    "            # Use NumPy for faster sorting\n",
    "            mz_array = np.array(ms2_mz)\n",
    "            intensity_array = np.array(ms2_intensity)\n",
    "            sorted_indices = np.argsort(mz_array)\n",
    "            mz_intensity_pairs = list(zip(mz_array[sorted_indices], intensity_array[sorted_indices]))\n",
    "\n",
    "            for row in annotations:\n",
    "                combined_multiple_samples.append({\n",
    "                    'cluster_id': row['cluster_id'],\n",
    "                    'file_id': row['file_id'],\n",
    "                    'feature_id': row['feature_id'],\n",
    "                    'peak_id': row['peak_id'],\n",
    "                    'msms_id': row['msms_id'],\n",
    "                    'ms2_rt': ms2_rt,\n",
    "                    'ms2_mz_intensity_pairs': mz_intensity_pairs,\n",
    "                    'charge_state': row['charge_state'],\n",
    "                    'ms2_precursor_info': ms2_precursor_info\n",
    "                })\n",
    "\n",
    "    return combined_multiple_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3356fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def build_annotations_lookup(df):\n",
    "    \"\"\"Create a dictionary for fast lookup by (file_id, msms_id).\"\"\"\n",
    "    annotations_lookup = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.notnull(row['msms_id']):\n",
    "            key = (row['file_id'], row['msms_id'])\n",
    "            annotations_lookup[key].append(row.to_dict())  # Use dicts for serialization\n",
    "    return annotations_lookup\n",
    "\n",
    "def process_file(file, output_dir, annotations_lookup):\n",
    "    import pastaq as pq\n",
    "    \n",
    "    results = []\n",
    "    if 'stem' not in file:\n",
    "        file['stem'] = os.path.splitext(os.path.basename(file['raw_path']))[0]\n",
    "    stem = file['stem']\n",
    "    in_path = os.path.join(output_dir, 'raw', f\"{stem}.ms2\")\n",
    "\n",
    "    if not os.path.exists(in_path):\n",
    "        return results\n",
    "\n",
    "    raw_data = pq.read_raw_data(in_path)\n",
    "\n",
    "    for scan in raw_data.scans:\n",
    "        scan_number = scan.scan_number\n",
    "        key = (stem, scan_number)\n",
    "        annotations = annotations_lookup.get(key)\n",
    "\n",
    "        if not annotations:\n",
    "            continue\n",
    "\n",
    "        ms2_mz = scan.mz\n",
    "        ms2_intensity = scan.intensity\n",
    "        ms2_rt = scan.retention_time\n",
    "        ms2_precursor_info = scan.precursor_information\n",
    "\n",
    "        if not ms2_mz or not ms2_intensity or len(ms2_mz) != len(ms2_intensity):\n",
    "            continue\n",
    "\n",
    "        # NumPy sorting\n",
    "        mz_array = np.array(ms2_mz)\n",
    "        intensity_array = np.array(ms2_intensity)\n",
    "        sorted_indices = np.argsort(mz_array)\n",
    "        mz_intensity_pairs = list(zip(mz_array[sorted_indices], intensity_array[sorted_indices]))\n",
    "\n",
    "        for row in annotations:\n",
    "            results.append({\n",
    "                'cluster_id': row['cluster_id'],\n",
    "                'file_id': row['file_id'],\n",
    "                'feature_id': row['feature_id'],\n",
    "                'peak_id': row['peak_id'],\n",
    "                'msms_id': row['msms_id'],\n",
    "                'ms2_rt': ms2_rt,\n",
    "                'ms2_mz_intensity_pairs': mz_intensity_pairs,\n",
    "                'charge_state': row['charge_state'],\n",
    "                'ms2_precursor_info': ms2_precursor_info\n",
    "            })\n",
    "    return results\n",
    "\n",
    "def combine_multiple_samples(feature_clusters_annotations_csv, input_files, output_dir, max_workers=4):\n",
    "    annotations_lookup = build_annotations_lookup(feature_clusters_annotations_csv)\n",
    "\n",
    "    combined_multiple_samples = []\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_file, file, output_dir, annotations_lookup)\n",
    "            for file in input_files\n",
    "        ]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            combined_multiple_samples.extend(future.result())\n",
    "\n",
    "    return combined_multiple_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5cfc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Preprocess reference data once\n",
    "reference_mz = np.array(reference_mz)\n",
    "reference_intensity = np.array(reference_intensity, dtype=np.float32)\n",
    "\n",
    "# Sort sample_mz_values if not already sorted\n",
    "sample_mz_values = np.sort(np.array(sample_mz_values))\n",
    "\n",
    "# Use searchsorted with sorted arrays\n",
    "matched_indices = np.searchsorted(reference_mz, sample_mz_values)\n",
    "\n",
    "# Clip indices to ensure they are within bounds\n",
    "matched_indices = np.clip(matched_indices, 0, len(reference_mz) - 1)\n",
    "\n",
    "# Extract corresponding intensities\n",
    "sample_spectrum = np.array(sample_maxima_intensities[:len(matched_indices)])\n",
    "reference_spectrum = reference_intensity[matched_indices]\n",
    "\n",
    "# Normalize spectra\n",
    "sample_spectrum_norm = sample_spectrum / np.linalg.norm(sample_spectrum)\n",
    "reference_spectrum_norm = reference_spectrum / np.linalg.norm(reference_spectrum)\n",
    "\n",
    "# Compute dot product\n",
    "dot_product = np.dot(sample_spectrum_norm, reference_spectrum_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e776828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_data = pq.read_msp_file(msp_file)\n",
    "filtered_msp = [\n",
    "    ann for ann in msp_data\n",
    "    if 'peaks' in ann and 'precursor_mz' in ann and 'retention_time' in ann\n",
    "]\n",
    "lib_mzs = np.array([ann['precursor_mz'] for ann in filtered_msp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27548f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_data = pq.read_msp_file(msp_file)\n",
    "filtered_msp = [\n",
    "    ann for ann in msp_data\n",
    "    if 'peaks' in ann and 'precursor_mz' in ann and 'retention_time' in ann\n",
    "]\n",
    "lib_mzs = np.array([ann['precursor_mz'] for ann in filtered_msp])\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "tree = KDTree(lib_mzs.reshape(-1, 1))\n",
    "\n",
    "idxs = tree.query_ball_point([[ms1_mz]], r=mz_tolerance)[0]\n",
    "candidates = [filtered_msp[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "tree = KDTree(lib_mzs.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cdd9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = tree.query_ball_point([[ms1_mz]], r=mz_tolerance)[0]\n",
    "candidates = [filtered_msp[i] for i in idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdf6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_data = pq.read_msp_file(msp_file)\n",
    "filtered_msp = [\n",
    "  ann for ann in msp_data\n",
    "  if 'peaks' in ann and 'precursor_mz' in ann and 'retention_time' in ann\n",
    "]\n",
    "\n",
    "precursors = np.array([ann['precursor_mz'] for ann in filtered_msp])\n",
    "rts = np.array([ann['retention_time'] for ann in filtered_msp])\n",
    "mask = (np.abs(precursors - ms1_mz) <= mz_tolerance) & (np.abs(rts - ms1_rt) <= rt_tolerance)\n",
    "candidates = [filtered_msp[i] for i in np.where(mask)[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_data = pq.read_msp_file(msp_file)\n",
    "filtered_msp = [\n",
    "  ann for ann in msp_data\n",
    "  if 'peaks' in ann and 'precursor_mz' in ann and 'retention_time' in ann\n",
    "]\n",
    "\n",
    "precursors = np.array([ann['precursor_mz'] for ann in filtered_msp])\n",
    "rts = np.array([ann['retention_time'] for ann in filtered_msp])\n",
    "mask = (np.abs(precursors - ms1_mz) <= mz_tolerance) & (np.abs(rts - ms1_rt) <= rt_tolerance)\n",
    "candidates = [filtered_msp[i] for i in np.where(mask)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d3d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "precursors = np.array([ann['precursor_mz'] for ann in filtered_msp])\n",
    "rts = np.array([ann['retention_time'] for ann in filtered_msp])\n",
    "mask = (np.abs(precursors - ms1_mz) <= mz_tolerance) & (np.abs(rts - ms1_rt) <= rt_tolerance)\n",
    "candidates = [filtered_msp[i] for i in np.where(mask)[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76dee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroided_peaks = me.clean_spectrum(ms2_sample_peaks, ...)\n",
    "cent_mz, cent_int = zip(*centroided_peaks)\n",
    "# reuse cent_mz, cent_int for all candidate MSP annotations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PASTAQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
