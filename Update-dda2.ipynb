{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f014ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import ms_entropy as me\n",
    "\n",
    "def combine_multiple_samples(feature_clusters_annotations_csv, input_files, output_dir):\n",
    "    # Preprocess: build a lookup dictionary to avoid filtering the DataFrame each time\n",
    "    annotations_lookup = defaultdict(list)\n",
    "\n",
    "    for _, row in feature_clusters_annotations_csv.iterrows():\n",
    "        if pd.notnull(row['msms_id']):\n",
    "            key = (row['file_id'], row['msms_id'])\n",
    "            annotations_lookup[key].append(row)\n",
    "\n",
    "    combined_multiple_samples = []\n",
    "\n",
    "    for file in input_files:\n",
    "        if 'stem' not in file:\n",
    "            base_name = os.path.splitext(os.path.basename(file['raw_path']))[0]\n",
    "            file['stem'] = base_name\n",
    "        stem = file['stem']\n",
    "        in_path = os.path.join(output_dir, 'raw', f\"{stem}.ms2\")\n",
    "\n",
    "        if not os.path.exists(in_path):\n",
    "            continue\n",
    "\n",
    "        raw_data = pq.read_raw_data(in_path)\n",
    "\n",
    "        for scan in raw_data.scans:\n",
    "            scan_number = scan.scan_number\n",
    "            key = (stem, scan_number)\n",
    "            annotations = annotations_lookup.get(key)\n",
    "\n",
    "            if not annotations:\n",
    "                continue\n",
    "\n",
    "            ms2_mz = scan.mz\n",
    "            ms2_intensity = scan.intensity\n",
    "            ms2_rt = scan.retention_time\n",
    "\n",
    "            if not ms2_mz or not ms2_intensity or len(ms2_mz) != len(ms2_intensity):\n",
    "                continue\n",
    "\n",
    "            # Convert to numpy array for faster sorting\n",
    "            mz_array = np.array(ms2_mz)\n",
    "            intensity_array = np.array(ms2_intensity)\n",
    "            sorted_indices = np.argsort(mz_array)\n",
    "            mz_intensity_pairs = list(zip(mz_array[sorted_indices], intensity_array[sorted_indices]))\n",
    "            ms2_peaks = np.array(mz_intensity_pairs, dtype=np.float32)\n",
    "\n",
    "            for row in annotations:\n",
    "                combined_multiple_samples.append({\n",
    "                    'cluster_id': row['cluster_id'],\n",
    "                    'file_id': row['file_id'],\n",
    "                    'feature_id': row['feature_id'],\n",
    "                    'peak_id': row['peak_id'],\n",
    "                    'msms_id': row['msms_id'],\n",
    "                    'ms2_rt': ms2_rt,\n",
    "                    'charge_state': row['charge_state'],\n",
    "                    'ms2_peaks' : ms2_peaks\n",
    "                })\n",
    "\n",
    "\n",
    "    return combined_multiple_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import ms_entropy as me\n",
    "from pathlib import Path\n",
    "\n",
    "# Fast & working!\n",
    "def link_features(combined_multiple_samples, input_files, output_dir):\n",
    "    # Preprocess: build a lookup dictionary to avoid filtering the DataFrame each time\n",
    "    annotations_lookup = defaultdict(list)\n",
    "    linked_features = []\n",
    "\n",
    "    for item in combined_multiple_samples:\n",
    "        if pd.notnull(item['feature_id']):\n",
    "            key = (item['file_id'], item['feature_id'])\n",
    "            annotations_lookup[key].append(item)\n",
    "\n",
    "    for file in input_files:\n",
    "        if 'stem' not in file:\n",
    "            base_name = os.path.splitext(os.path.basename(file['raw_path']))[0]\n",
    "            file['stem'] = base_name\n",
    "        stem = file['stem']\n",
    "        in_path_features = os.path.join(output_dir, 'features', f\"{stem}.features\")\n",
    "\n",
    "        if not os.path.exists(in_path_features):\n",
    "            print('missing feature file/s')\n",
    "            continue\n",
    "\n",
    "        features = pq.read_features(in_path_features)\n",
    "\n",
    "        for feature in features:\n",
    "            id = feature.id\n",
    "            key = (stem, id)\n",
    "            annotations = annotations_lookup.get(key)\n",
    "\n",
    "            if not annotations:\n",
    "                continue\n",
    "\n",
    "            if isinstance(annotations, list):\n",
    "                for annotation in annotations:\n",
    "                    linked_features.append({\n",
    "                        'cluster_id': annotation['cluster_id'],\n",
    "                        'file_id': annotation['file_id'],\n",
    "                        'feature_id': id,\n",
    "                        'feature_peak_ids': feature.peak_ids,\n",
    "                        'peak_id' : annotation['peak_id'],\n",
    "                        'msms_id': annotation['msms_id'],\n",
    "                        'precursor_mz' : feature.monoisotopic_mz,\n",
    "                        'precursor_rt': feature.monoisotopic_rt,\n",
    "                        'precursor_intensity': feature.monoisotopic_height,\n",
    "                        'precursor_vol': feature.monoisotopic_volume,\n",
    "                        'total_intensity': feature.total_height,\n",
    "                        'total_volume': feature.total_volume,\n",
    "                        'average_ms1_mz': feature.average_mz,\n",
    "                        'average_rt': feature.average_rt,\n",
    "                        'charge_state': feature.charge_state,\n",
    "                        'ms2_sample_peaks': annotation['ms2_peaks'],                  \n",
    "                        })\n",
    "\n",
    "    return linked_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9cd45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import ms_entropy as me\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import ast\n",
    "\n",
    "# Fast and working!\n",
    "def link_msp(linked_features, msp_data, mz_tolerance=0.025, rt_tolerance=8.0):\n",
    "    linked_msp_features = []\n",
    "\n",
    "    # Loop through a list of Feature objects\n",
    "    for feature in linked_features:\n",
    "        cluster_id = feature['cluster_id']\n",
    "        file_id = feature['file_id']\n",
    "        feature_id = feature['feature_id']\n",
    "        feature_peak_ids = feature['feature_peak_ids']\n",
    "        peak_id = feature['peak_id']\n",
    "        msms_id = feature['msms_id']\n",
    "        precursor_mz = feature['precursor_mz']\n",
    "        precursor_rt = feature['precursor_rt']\n",
    "        precursor_intensity = feature['precursor_intensity']\n",
    "        precursor_volume = feature['precursor_vol']\n",
    "        average_ms1_mz = feature['average_ms1_mz']\n",
    "        average_rt = feature['average_rt']\n",
    "        charge_state = feature['charge_state']\n",
    "        ms2_sample_peaks = feature['ms2_sample_peaks']\n",
    "        total_intensity = feature['total_intensity'] \n",
    "        total_volume = feature['total_volume']                   \n",
    "        normalized_area = precursor_volume * 114.7977026\n",
    "\n",
    "        # Centroid the MS2 spectrum\n",
    "        centroided_peaks = me.clean_spectrum(\n",
    "                    ms2_sample_peaks,\n",
    "                    min_ms2_difference_in_da=0.02,\n",
    "                    normalize_intensity=False\n",
    "                )\n",
    "\n",
    "        centroided_arr = np.array(centroided_peaks)\n",
    "        centroided_arr_list = centroided_arr.tolist()\n",
    "\n",
    "        # Find all matching annotations for the current scan\n",
    "        best_match = None  # To store the best match found\n",
    "\n",
    "        for annotation in msp_data:\n",
    "            # Ensure required fields are present in the annotation\n",
    "            if 'precursor_mz' not in annotation or 'retention_time' not in annotation:\n",
    "                continue  # Skip this annotation\n",
    "\n",
    "            # Calculate mz and rt distances directly for scalars\n",
    "            mz_distance = np.abs(precursor_mz - annotation['precursor_mz']) # changed this from 'mz' to 'precursor_mz'\n",
    "            rt_distance = np.abs(precursor_rt - annotation['retention_time'])  # changed this from 'retention_time' to 'precursor_rt' \n",
    "\n",
    "            # Apply the tolerance checks\n",
    "            if mz_distance <= mz_tolerance and rt_distance <= rt_tolerance:\n",
    "                # If the distances are within tolerance, calculate the match score with normalization factor\n",
    "                match_score = (rt_distance*0.025) + (mz_distance*20) # lower score is better/ closer match; added weighting\n",
    "                \n",
    "                # Calculate mass error in ppm using formula\n",
    "                mass_error_ppm = ((annotation['precursor_mz'] - precursor_mz) / annotation['precursor_mz']) * 10**6\n",
    "                \n",
    "                #Calculate root mean squared error for best match\n",
    "                y_true_mz = [annotation.get('precursor_mz')]\n",
    "                y_pred_mz = [precursor_mz]\n",
    "                rmse_mz = root_mean_squared_error(y_true_mz, y_pred_mz)\n",
    "\n",
    "                # Convert peaks to numpy arrays for similarity calculation (MS_Entropy)\n",
    "                peaks_query = np.array(ms2_sample_peaks, dtype=np.float32) #peaks from given samples\n",
    "                peaks_reference = np.array([annotation.get('peaks')], dtype=np.float32)  # peaks from msp\n",
    "                if peaks_reference.ndim == 3 and peaks_reference.shape[0] == 1:\n",
    "                    peaks_reference = peaks_reference[0]  # remove the first singleton dimension\n",
    "                    \n",
    "                # Calculate similarity scores (MS_entropy)\n",
    "                unweighted_similarity = me.calculate_unweighted_entropy_similarity(peaks_query, peaks_reference)\n",
    "                similarity = me.calculate_entropy_similarity(peaks_query, peaks_reference) # entropy based intensity weights are applied to the peaks\n",
    "\n",
    "                # Extract m/z and intensity values\n",
    "                reference_mz, reference_intensity = zip(*peaks_reference)\n",
    "\n",
    "                # Convert to NumPy arrays\n",
    "                # Parse the mz and intensity arrays\n",
    "                sample_mz, sample_intensity = zip(*centroided_peaks)\n",
    "                sample_mz = np.array(sample_mz)\n",
    "                sample_intensity = np.array(sample_intensity, dtype=np.float32)\n",
    "                reference_mz = np.array(reference_mz)\n",
    "                reference_intensity = np.array(reference_intensity, dtype=np.float32)\n",
    "\n",
    "                # Match peaks based on m/z values\n",
    "                matched_indices = np.searchsorted(reference_mz, sample_mz)\n",
    "\n",
    "                # Ensure indices are within bounds (removes any indices that are out of bounds)\n",
    "                matched_indices = matched_indices[matched_indices < len(reference_mz)]\n",
    "\n",
    "                # Example mass spectra intensities\n",
    "                sample_spectrum = np.array(sample_intensity[:len(matched_indices)])\n",
    "                reference_spectrum = np.array(reference_intensity[matched_indices])\n",
    "\n",
    "                # Normalize both vectors to unit length (L2 norm)\n",
    "                sample_spectrum_normalized = sample_spectrum / np.linalg.norm(sample_spectrum)\n",
    "                reference_spectrum_normalized = reference_spectrum / np.linalg.norm(reference_spectrum)\n",
    "\n",
    "                # Compute dot product (cosine similarity if normalized)\n",
    "                if len(sample_intensity) < 3 or len(reference_intensity) < 3:\n",
    "                    dot_product = None\n",
    "                else:\n",
    "                    dot_product = np.dot(sample_spectrum_normalized, reference_spectrum_normalized)\n",
    "                \n",
    "                if best_match is None or match_score < best_match['score']:\n",
    "                    best_match = {\n",
    "                        'score': float(match_score),  # Store the match score\n",
    "                        'name': annotation.get('name', None),\n",
    "                        'saturation' : annotation.get('saturation', None),\n",
    "                        'retention_time': annotation.get('retention_time', None),\n",
    "                        'precursor_mz': annotation.get('precursor_mz', None),\n",
    "                        'precursor_type': annotation.get('precursor_type', None),\n",
    "                        'smiles': annotation.get('smiles', None),\n",
    "                        'msp_peaks': annotation.get('peaks', None),\n",
    "                    }\n",
    "        \n",
    "                    # Create the annotated scan with only the best match\n",
    "                    linked_msp_feature = {\n",
    "                        'cluster_id' : cluster_id,\n",
    "                        'file_id' : file_id,\n",
    "                        'feature_id': feature_id,\n",
    "                        'feature_peak_ids' : feature_peak_ids,\n",
    "                        'peak_id': peak_id,\n",
    "                        'msms_id' : msms_id,\n",
    "                        'average_ms1_mz': average_ms1_mz,\n",
    "                        'total_intensity': total_intensity,  # Intensity values\n",
    "                        'average_rt': average_rt,  # Average retention time (in seconds)\n",
    "                        'total_volume' : total_volume,\n",
    "                        'precursor_mz': precursor_mz,\n",
    "                        'precursor_intensity' : precursor_intensity,\n",
    "                        'precursor_rt' : precursor_rt,\n",
    "                        'precursor_volume' : precursor_volume,\n",
    "                        'normalized_area' : normalized_area,\n",
    "                        'charge_state' : charge_state,\n",
    "                        'centroided_ms2_peaks' : centroided_arr_list,\n",
    "                        'mass_error_ppm' : float(mass_error_ppm),\n",
    "                        'rmse_mz' : float(rmse_mz),\n",
    "                        'unweighted_entropy_similarity' : unweighted_similarity,\n",
    "                        'entropy_similarity' : similarity,\n",
    "                        'dot_product': float(dot_product) if dot_product is not None else 'NA',\n",
    "                        'matches': []  # List to hold the top match\n",
    "                        }\n",
    "\n",
    "        # Add the best match if available\n",
    "        if best_match:\n",
    "            linked_msp_feature['matches'].append(best_match)\n",
    "        else:\n",
    "            linked_msp_feature['matches'] = None  # No match found\n",
    "        \n",
    "        linked_msp_features.append(linked_msp_feature)\n",
    "\n",
    "    # Return after all features are processed\n",
    "    return linked_msp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import ms_entropy as me\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import ast\n",
    "\n",
    "# Fast and working!\n",
    "def link_msp(linked_features, msp_data, mz_tolerance=0.025, rt_tolerance=8.0):\n",
    "    linked_msp_features = []\n",
    "\n",
    "    # Loop through a list of Feature objects\n",
    "    for feature in linked_features:\n",
    "        cluster_id = feature['cluster_id']\n",
    "        file_id = feature['file_id']\n",
    "        feature_id = feature['feature_id']\n",
    "        feature_peak_ids = feature['feature_peak_ids']\n",
    "        peak_id = feature['peak_id']\n",
    "        msms_id = feature['msms_id']\n",
    "        precursor_mz = feature['precursor_mz']\n",
    "        precursor_rt = feature['precursor_rt']\n",
    "        precursor_intensity = feature['precursor_intensity']\n",
    "        precursor_volume = feature['precursor_vol']\n",
    "        average_ms1_mz = feature['average_ms1_mz']\n",
    "        average_rt = feature['average_rt']\n",
    "        charge_state = feature['charge_state']\n",
    "        ms2_sample_peaks = feature['ms2_sample_peaks']\n",
    "        total_intensity = feature['total_intensity']\n",
    "        total_volume = feature['total_volume']\n",
    "        normalized_area = precursor_volume * 114.7977026\n",
    "\n",
    "        # Centroid the MS2 spectrum\n",
    "        centroided_peaks = me.clean_spectrum(\n",
    "            ms2_sample_peaks,\n",
    "            min_ms2_difference_in_da=0.02,\n",
    "            normalize_intensity=False\n",
    "        )\n",
    "\n",
    "        centroided_arr = np.array(centroided_peaks)\n",
    "        centroided_arr_list = centroided_arr.tolist()\n",
    "\n",
    "        # Find all matching annotations for the current scan\n",
    "        best_match = None\n",
    "\n",
    "        for annotation in msp_data:\n",
    "            if 'precursor_mz' not in annotation or 'retention_time' not in annotation:\n",
    "                continue\n",
    "\n",
    "            # Calculate mz and rt distances directly for scalars\n",
    "            mz_distance = np.abs(precursor_mz - annotation['precursor_mz'])\n",
    "            rt_distance = np.abs(precursor_rt - annotation['retention_time'])\n",
    "\n",
    "            if mz_distance <= mz_tolerance and rt_distance <= rt_tolerance:\n",
    "                match_score = (rt_distance * 0.025) + (mz_distance * 20)\n",
    "                mass_error_ppm = ((annotation['precursor_mz'] - precursor_mz) / annotation['precursor_mz']) * 1e6\n",
    "\n",
    "                y_true_mz = [annotation.get('precursor_mz')]\n",
    "                y_pred_mz = [precursor_mz]\n",
    "                rmse_mz = root_mean_squared_error(y_true_mz, y_pred_mz)\n",
    "\n",
    "                peaks_query = np.array(ms2_sample_peaks, dtype=np.float32)\n",
    "                peaks_reference = np.array([annotation.get('peaks')], dtype=np.float32)\n",
    "                if peaks_reference.ndim == 3 and peaks_reference.shape[0] == 1:\n",
    "                    peaks_reference = peaks_reference[0]\n",
    "\n",
    "                unweighted_similarity = me.calculate_unweighted_entropy_similarity(peaks_query, peaks_reference)\n",
    "                similarity = me.calculate_entropy_similarity(peaks_query, peaks_reference)\n",
    "\n",
    "                reference_mz, reference_intensity = zip(*peaks_reference)\n",
    "                sample_mz, sample_intensity = zip(*centroided_peaks)\n",
    "\n",
    "                sample_mz = np.array(sample_mz)\n",
    "                sample_intensity = np.array(sample_intensity, dtype=np.float32)\n",
    "                reference_mz = np.array(reference_mz)\n",
    "                reference_intensity = np.array(reference_intensity, dtype=np.float32)\n",
    "\n",
    "                matched_indices = np.searchsorted(reference_mz, sample_mz)\n",
    "                matched_indices = matched_indices[matched_indices < len(reference_mz)]\n",
    "\n",
    "                sample_spectrum = sample_intensity[:len(matched_indices)]\n",
    "                reference_spectrum = reference_intensity[matched_indices]\n",
    "\n",
    "                sample_spectrum_normalized = sample_spectrum / np.linalg.norm(sample_spectrum)\n",
    "                reference_spectrum_normalized = reference_spectrum / np.linalg.norm(reference_spectrum)\n",
    "\n",
    "                dot_product = None\n",
    "                if len(sample_intensity) >= 3 and len(reference_intensity) >= 3:\n",
    "                    dot_product = np.dot(sample_spectrum_normalized, reference_spectrum_normalized)\n",
    "\n",
    "                if best_match is None or match_score < best_match['score']:\n",
    "                    best_match = {\n",
    "                        'score': float(match_score),\n",
    "                        'name': annotation.get('name', None),\n",
    "                        'saturation': annotation.get('saturation', None),\n",
    "                        'retention_time': annotation.get('retention_time', None),\n",
    "                        'precursor_mz': annotation.get('precursor_mz', None),\n",
    "                        'precursor_type': annotation.get('precursor_type', None),\n",
    "                        'smiles': annotation.get('smiles', None),\n",
    "                        'msp_peaks': annotation.get('peaks', None),\n",
    "                    }\n",
    "\n",
    "        linked_msp_feature = {\n",
    "            'cluster_id': cluster_id,\n",
    "            'file_id': file_id,\n",
    "            'feature_id': feature_id,\n",
    "            'feature_peak_ids': feature_peak_ids,\n",
    "            'peak_id': peak_id,\n",
    "            'msms_id': msms_id,\n",
    "            'average_ms1_mz': average_ms1_mz,\n",
    "            'total_intensity': total_intensity,\n",
    "            'average_rt': average_rt,\n",
    "            'total_volume': total_volume,\n",
    "            'precursor_mz': precursor_mz,\n",
    "            'precursor_intensity': precursor_intensity,\n",
    "            'precursor_rt': precursor_rt,\n",
    "            'precursor_volume': precursor_volume,\n",
    "            'normalized_area': normalized_area,\n",
    "            'charge_state': charge_state,\n",
    "            'centroided_ms2_peaks': centroided_arr_list,\n",
    "            'mass_error_ppm': float(mass_error_ppm) if best_match else None,\n",
    "            'rmse_mz': float(rmse_mz) if best_match else None,\n",
    "            'unweighted_entropy_similarity': unweighted_similarity if best_match else None,\n",
    "            'entropy_similarity': similarity if best_match else None,\n",
    "            'dot_product': float(dot_product) if dot_product is not None else 'NA',\n",
    "            'matches': [best_match] if best_match else None,\n",
    "            'raw_ms2_peaks' : peaks_query,\n",
    "            'msp_peaks' : peaks_reference\n",
    "        }\n",
    "\n",
    "        linked_msp_features.append(linked_msp_feature)\n",
    "\n",
    "    return linked_msp_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "def dot_product_with_tolerance(mz1, int1, mz2, int2, tol=0.02):\n",
    "    matched1, matched2 = [], []\n",
    "\n",
    "    for i, m1 in enumerate(mz1):\n",
    "        for j, m2 in enumerate(mz2):\n",
    "            if abs(m1 - m2) <= tol:\n",
    "                matched1.append(int1[i])\n",
    "                matched2.append(int2[j])\n",
    "                break  # prevent duplicate matching\n",
    "\n",
    "    if not matched1 or not matched2:\n",
    "        return 0.0\n",
    "\n",
    "    # Convert into numpy arrays\n",
    "    s1 = np.array(matched1)\n",
    "    s2 = np.array(matched2)\n",
    "    # Normalization: each vector is divided by its Euclidean norm to convert it into a unit vector.\n",
    "    s1 /= np.linalg.norm(s1)\n",
    "    s2 /= np.linalg.norm(s2)\n",
    "    # Calculate dot product\n",
    "    return float(np.dot(s1, s2))\n",
    "\n",
    "# Calculate dot products\n",
    "results = []\n",
    "for peak_id, group in df.groupby('peak_id'):\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "    for (i1, row1), (i2, row2) in combinations(group.iterrows(), 2):\n",
    "        dot_product = dot_product_with_tolerance(row1['cent_mz'], row1['cent_intensity'],\n",
    "                                        row2['cent_mz'], row2['cent_intensity'])\n",
    "        unweighted_similarity = me.calculate_unweighted_entropy_similarity(row1['ms2_peaks'], row2['ms2_peaks'])\n",
    "        similarity = me.calculate_entropy_similarity(row1['ms2_peaks'], row2['ms2_peaks'])\n",
    "        results.append({\n",
    "            'dot_product': dot_product,\n",
    "            'unweighted_entropy_similarity' : unweighted_similarity,\n",
    "            'entropy_similarity' : similarity,\n",
    "            'peak_id': peak_id,\n",
    "            'msms_id_1': row1['msms_id'],\n",
    "            'mz1' : row1['cent_mz'],\n",
    "            'int1' : row1['cent_intensity'],\n",
    "            'feature_id1' : row1['feature_id'],\n",
    "            'cluster_id1' : row1['cluster_id'],\n",
    "            'file_id1' : row1['file_id'],\n",
    "            'msms_id_2': row2['msms_id'],\n",
    "            'mz2' : row2['cent_mz'],\n",
    "            'int2' : row2['cent_intensity'],\n",
    "            'feature_id2' : row2['feature_id'],\n",
    "            'cluster_id2' : row2['cluster_id'],\n",
    "            'file_id2' : row2['file_id']\n",
    "        })\n",
    "\n",
    "# Create the results DataFrame\n",
    "dot_product_df = pd.DataFrame(results)\n",
    "print(dot_product_df.head())\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea499f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# WORKING - not debug\n",
    "def parse_array_from_string(s):\n",
    "    if isinstance(s, str):\n",
    "        return np.array([float(x) for x in re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", s)])\n",
    "    return np.array([])\n",
    "\n",
    "def read_msp_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    spectra_data = []\n",
    "    current_spectrum = {}\n",
    "    peak_data_started = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"Num Peaks\"):\n",
    "            peak_data_started = True\n",
    "            continue\n",
    "\n",
    "        if not peak_data_started:\n",
    "            if line.startswith(\"NAME:\"):\n",
    "                if \"|\" in line:\n",
    "                    # If the line contains '|', split the line after the '|' character\n",
    "                    parts = line.split('|')\n",
    "                    current_spectrum['name'] = parts[0].replace(\"NAME:\", \"\").strip()  # Remove \"NAME:\" and strip any extra spaces\n",
    "                    current_spectrum['saturation'] = parts[1].strip()  # After the '|'\n",
    "                else:\n",
    "                    # Otherwise, just use the name\n",
    "                    current_spectrum['name'] = line.split(\":\", 1)[1].strip()\n",
    "               \n",
    "            elif line.startswith(\"PRECURSORMZ:\"):\n",
    "                current_spectrum['precursor_mz'] = float(line.split(\":\", 1)[1].strip())\n",
    "            elif line.startswith(\"PRECURSORTYPE:\"):\n",
    "                current_spectrum['precursor_type'] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"IONMODE:\"):\n",
    "                current_spectrum['ion_mode'] = line.split(\":\", 1)[1].strip()              \n",
    "            elif line.startswith(\"RETENTIONTIME:\"):\n",
    "                _, raw = line.split(\":\", 1)\n",
    "                val = raw.strip()\n",
    "                if not val:\n",
    "                    print(\"[SKIP] empty retention_time\")\n",
    "                    continue\n",
    "                try:\n",
    "                    rt = float(val)\n",
    "                except ValueError:\n",
    "                        rt_parsed = parse_array_from_string(val)\n",
    "                        if isinstance(rt_parsed, (list, np.ndarray)) and len(rt_parsed) == 1:\n",
    "                            rt = float(rt_parsed[0])\n",
    "                        else:\n",
    "                            print(\"[SKIP] array invalid, skipping\")\n",
    "                            continue\n",
    "                current_spectrum['retention_time'] = rt * 60 # convert to seconds\n",
    "            elif line.startswith(\"Name: \"):\n",
    "                if current_spectrum:\n",
    "                    spectra_data.append(current_spectrum)\n",
    "                    current_spectrum = {\"Name\": line.split(\":\",1)[1].strip()}\n",
    "            elif line.startswith(\"FORMULA:\"):\n",
    "                current_spectrum['formula'] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"INCHIKEY:\"):\n",
    "                current_spectrum['inchi_key'] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"SMILES:\"):\n",
    "                current_spectrum['smiles'] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"COMMENT:\"):\n",
    "                current_spectrum['comment'] = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                mz, intensity = map(float, line.split())\n",
    "                current_spectrum.setdefault('peaks', []).append((mz, intensity))\n",
    "            except ValueError:\n",
    "                # This is where the spectrum data is stored and new spectrum begins\n",
    "                if current_spectrum:\n",
    "                    spectra_data.append(current_spectrum)\n",
    "                current_spectrum = {}  # Reset for the next spectrum\n",
    "                peak_data_started = False  # Reset peak reading flag\n",
    "\n",
    "    # Add last spectrum if it exists\n",
    "    if current_spectrum:\n",
    "        spectra_data.append(current_spectrum)\n",
    "    \n",
    "    return spectra_data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
