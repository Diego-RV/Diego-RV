{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3818f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pastaq as pq\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518ce7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [{'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE20_exc0-p1_2.ms2\"},\n",
    "               { 'raw_path': r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE20_exc2-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE20_exc10-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE30_exc0-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE30_exc2-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE30_exc10-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE40_exc0-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE40_exc2-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE40_exc10-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE50_exc0-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE50_exc2-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE50_exc10-p1_2.ms2\"},\n",
    "                {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE60_exc0-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE60_exc2-p1_2.ms2\"},\n",
    "               {'raw_path' : r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\raw\\p_CE60_exc10-p1_2.ms2\"},\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc9be05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dac704e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_clusters_annotations_csv = pd.read_csv(r\"C:\\Users\\diego.DESKTOP-7OSFK5B\\Documents\\MSc_Research_Project1\\CID_files\\pastaq_CID\\quant\\feature_clusters_annotations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56e673a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import ms_entropy as me\n",
    "\n",
    "def combine_multiple_samples(feature_clusters_annotations_csv, input_files, output_dir):\n",
    "    # Preprocess: build a lookup dictionary to avoid filtering the DataFrame each time\n",
    "    annotations_lookup = defaultdict(list)\n",
    "\n",
    "    for _, row in feature_clusters_annotations_csv.iterrows():\n",
    "        if pd.notnull(row['msms_id']):\n",
    "            key = (row['file_id'], row['msms_id'])\n",
    "            annotations_lookup[key].append(row)\n",
    "\n",
    "    combined_multiple_samples = []\n",
    "\n",
    "    for file in input_files:\n",
    "        if 'stem' not in file:\n",
    "            base_name = os.path.splitext(os.path.basename(file['raw_path']))[0]\n",
    "            file['stem'] = base_name\n",
    "        stem = file['stem']\n",
    "        in_path = os.path.join(output_dir, 'raw', f\"{stem}.ms2\")\n",
    "\n",
    "        if not os.path.exists(in_path):\n",
    "            continue\n",
    "\n",
    "        raw_data = pq.read_raw_data(in_path)\n",
    "\n",
    "        for scan in raw_data.scans:\n",
    "            scan_number = scan.scan_number\n",
    "            key = (stem, scan_number)\n",
    "            annotations = annotations_lookup.get(key)\n",
    "\n",
    "            if not annotations:\n",
    "                continue\n",
    "\n",
    "            ms2_mz = scan.mz\n",
    "            ms2_intensity = scan.intensity\n",
    "            ms2_rt = scan.retention_time\n",
    "\n",
    "            if not ms2_mz or not ms2_intensity or len(ms2_mz) != len(ms2_intensity):\n",
    "                continue\n",
    "\n",
    "            # Convert to numpy array for faster sorting\n",
    "            mz_array = np.array(ms2_mz)\n",
    "            intensity_array = np.array(ms2_intensity)\n",
    "            sorted_indices = np.argsort(mz_array)\n",
    "            mz_intensity_pairs = list(zip(mz_array[sorted_indices], intensity_array[sorted_indices]))\n",
    "            ms2_peaks = np.array(mz_intensity_pairs, dtype=np.float32)\n",
    "            centroided_peaks = me.clean_spectrum(\n",
    "                    ms2_peaks,\n",
    "                    min_ms2_difference_in_da=0.02,\n",
    "                    normalize_intensity=False\n",
    "                )\n",
    "            cent_mz, cent_intensity = zip(*centroided_peaks)\n",
    "            cent_mz_arr = np.array(cent_mz)\n",
    "            cent_mz_arr_list = cent_mz_arr.tolist()\n",
    "            cent_intensity_arr = np.array(cent_intensity)\n",
    "            cent_intensity_arr_list = cent_intensity_arr.tolist()\n",
    "\n",
    "            for row in annotations:\n",
    "                combined_multiple_samples.append({\n",
    "                    'cluster_id': row['cluster_id'],\n",
    "                    'file_id': row['file_id'],\n",
    "                    'feature_id': row['feature_id'],\n",
    "                    'peak_id': row['peak_id'],\n",
    "                    'msms_id': row['msms_id'],\n",
    "                    'ms2_rt': ms2_rt,\n",
    "                    'charge_state': row['charge_state'],\n",
    "                    'ms2_peaks' : ms2_peaks,\n",
    "                    'cent_mz' : cent_mz_arr_list,\n",
    "                    'cent_intensity' : cent_intensity_arr_list\n",
    "                })\n",
    "\n",
    "\n",
    "    return combined_multiple_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae6435ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_multiple_samples = combine_multiple_samples(feature_clusters_annotations_csv, input_files, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca40206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(combined_multiple_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73132fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import ms_entropy as me\n",
    "#Working version 1\n",
    "def dot_product_with_tolerance(mz1, int1, mz2, int2, tol=0.02):\n",
    "    matched1, matched2 = [], []\n",
    "    for i, m1 in enumerate(mz1):\n",
    "        for j, m2 in enumerate(mz2):\n",
    "            if abs(m1 - m2) <= tol:\n",
    "                matched1.append(int1[i])\n",
    "                matched2.append(int2[j])\n",
    "                break  # prevent duplicate matching\n",
    "    if not matched1 or not matched2:\n",
    "        return 0.0\n",
    "    s1 = np.array(matched1)\n",
    "    s2 = np.array(matched2)\n",
    "    s1 /= np.linalg.norm(s1)\n",
    "    s2 /= np.linalg.norm(s2)\n",
    "    return float(np.dot(s1, s2))\n",
    "\n",
    "def process_spectral_similarity(df, top_n=3, mz_tolerance=0.02, mz_merge_thresh=0.01):\n",
    "    required_columns = ['peak_id', 'cluster_id', 'file_id', 'feature_id', 'msms_id',\n",
    "                        'ms2_rt', 'charge_state', 'ms2_peaks', 'cent_mz', 'cent_intensity']\n",
    "    assert all(col in df.columns for col in required_columns), \"Missing required columns in df\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for peak_id, group in df.groupby('peak_id'):\n",
    "        if len(group) < 2:\n",
    "            row = group.iloc[0]\n",
    "            cent_mz_intensity_pairs = list(zip(row['cent_mz'], row['cent_intensity']))\n",
    "            results.append({\n",
    "                'cluster_id': row['cluster_id'],\n",
    "                'feature_id': row['feature_id'],\n",
    "                'peak_id': peak_id,\n",
    "                'avg_ms2_retention_time': row['ms2_rt'],\n",
    "                'num_msms': 1,\n",
    "                'charge_state': row['charge_state'],\n",
    "                'avg_ms2_cent_peaks': cent_mz_intensity_pairs,\n",
    "                'raw_ms2_sample_peaks': row['ms2_peaks']\n",
    "            })\n",
    "        else:\n",
    "            msms_list = []\n",
    "            for (i1, row1), (i2, row2) in combinations(group.iterrows(), 2):\n",
    "                dot_product = dot_product_with_tolerance(row1['cent_mz'], row1['cent_intensity'],\n",
    "                                                         row2['cent_mz'], row2['cent_intensity'],\n",
    "                                                         tol=mz_tolerance)\n",
    "                entropy_sim = me.calculate_entropy_similarity(row1['ms2_peaks'], row2['ms2_peaks'])\n",
    "\n",
    "                msms_list.append({\n",
    "                    'dot_product': dot_product,\n",
    "                    'entropy_similarity': entropy_sim,\n",
    "                    'ms2_retention_time': row1['ms2_rt'],\n",
    "                    'cent_mz_intensity_pairs': list(zip(row1['cent_intensity'], row1['cent_mz']))  # (intensity, mz)\n",
    "                })\n",
    "\n",
    "            if not msms_list:\n",
    "                continue\n",
    "\n",
    "            sorted_msms = sorted(msms_list, key=lambda x: x['entropy_similarity'], reverse=True)\n",
    "            top_msms = sorted_msms[:top_n]\n",
    "\n",
    "            dot_product_list = [msms['dot_product'] for msms in top_msms]\n",
    "            entropy_similarity_list = [msms['entropy_similarity'] for msms in top_msms]\n",
    "            avg_dot_product = np.mean(dot_product_list)\n",
    "            avg_entropy_similarity = np.mean(entropy_similarity_list)\n",
    "            avg_retention_time = np.mean([msms['ms2_retention_time'] for msms in top_msms])\n",
    "\n",
    "            # Merge centroids from top MS/MS\n",
    "            all_cent_ms2_peaks = [np.array(pairs) for msms in top_msms for pairs in [msms['cent_mz_intensity_pairs']]]\n",
    "            combined_cent_peaks = np.concatenate(all_cent_ms2_peaks, axis=0)\n",
    "            sorted_arr = combined_cent_peaks[combined_cent_peaks[:, 1].argsort()]\n",
    "\n",
    "            groups = []\n",
    "            current_group = [sorted_arr[0]]\n",
    "            for row in sorted_arr[1:]:\n",
    "                if abs(row[1] - current_group[-1][1]) <= mz_merge_thresh:\n",
    "                    current_group.append(row)\n",
    "                else:\n",
    "                    groups.append(np.array(current_group))\n",
    "                    current_group = [row]\n",
    "            groups.append(np.array(current_group))  # Add last group\n",
    "\n",
    "            averaged_results = []\n",
    "            for group in groups:\n",
    "                avg_intensity = np.mean(group[:, 0])\n",
    "                avg_mz = np.mean(group[:, 1])\n",
    "                averaged_results.append([avg_intensity, avg_mz])\n",
    "            avg_ms2_cent_peaks = np.array(averaged_results)\n",
    "\n",
    "            results.append({\n",
    "                'peak_id': peak_id,\n",
    "                'dot_product_list': dot_product_list,\n",
    "                'avg_dot_product': avg_dot_product,\n",
    "                'avg_ms2_retention_time': avg_retention_time,\n",
    "                'num_msms': len(top_msms),\n",
    "                'entropy_similarity_list': entropy_similarity_list,\n",
    "                'avg_entropy_similarity': avg_entropy_similarity,\n",
    "                'avg_ms2_cent_peaks': avg_ms2_cent_peaks.tolist()\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5198eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import ms_entropy as me\n",
    "#Version 2\n",
    "def dot_product_with_tolerance(mz1, int1, mz2, int2, tol=0.02):\n",
    "    matched1, matched2 = [], []\n",
    "    for i, m1 in enumerate(mz1):\n",
    "        for j, m2 in enumerate(mz2):\n",
    "            if abs(m1 - m2) <= tol:\n",
    "                matched1.append(int1[i])\n",
    "                matched2.append(int2[j])\n",
    "                break  # prevent duplicate matching\n",
    "    if not matched1 or not matched2:\n",
    "        return 0.0\n",
    "    s1 = np.array(matched1)\n",
    "    s2 = np.array(matched2)\n",
    "    s1 /= np.linalg.norm(s1)\n",
    "    s2 /= np.linalg.norm(s2)\n",
    "    return float(np.dot(s1, s2))\n",
    "\n",
    "def process_spectral_similarity(df, top_n=3, mz_tolerance=0.02, mz_merge_thresh=0.01):\n",
    "    required_columns = ['peak_id', 'cluster_id', 'file_id', 'feature_id', 'msms_id',\n",
    "                        'ms2_rt', 'charge_state', 'ms2_peaks', 'cent_mz', 'cent_intensity']\n",
    "    assert all(col in df.columns for col in required_columns), \"Missing required columns in df\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for peak_id, group in df.groupby('peak_id'):\n",
    "        if len(group) < 2:\n",
    "            row = group.iloc[0]\n",
    "            cent_mz_intensity_pairs = list(zip(row['cent_mz'], row['cent_intensity']))\n",
    "            results.append({\n",
    "                'cluster_id': row['cluster_id'],\n",
    "                'feature_id': row['feature_id'],\n",
    "                'peak_id': peak_id,\n",
    "                'avg_ms2_retention_time': row['ms2_rt'],\n",
    "                'num_msms': 1,\n",
    "                'charge_state': row['charge_state'],\n",
    "                'avg_ms2_cent_peaks': cent_mz_intensity_pairs,\n",
    "                'raw_ms2_sample_peaks': row['ms2_peaks']\n",
    "            })\n",
    "        else:\n",
    "            msms_list = []\n",
    "            for (i1, row1), (i2, row2) in combinations(group.iterrows(), 2):\n",
    "                dot_product = dot_product_with_tolerance(row1['cent_mz'], row1['cent_intensity'],\n",
    "                                                         row2['cent_mz'], row2['cent_intensity'],\n",
    "                                                         tol=mz_tolerance)\n",
    "                entropy_sim = me.calculate_entropy_similarity(row1['ms2_peaks'], row2['ms2_peaks'])\n",
    "\n",
    "                msms_list.append({\n",
    "                    'cluster_id' : row1['cluster_id'],\n",
    "                    'feature_id' : row1['feature_id'],\n",
    "                    'dot_product': dot_product,\n",
    "                    'entropy_similarity': entropy_sim,\n",
    "                    'ms2_retention_time': row1['ms2_rt'],\n",
    "                    'cent_mz_intensity_pairs': list(zip(row1['cent_mz'], row1['cent_intensity']))  #Fixed to (mz, intensity) from (intensity, mz)\n",
    "                })\n",
    "\n",
    "            if not msms_list:\n",
    "                continue\n",
    "\n",
    "            sorted_msms = sorted(msms_list, key=lambda x: x['entropy_similarity'], reverse=True)\n",
    "            top_msms = sorted_msms[:top_n]\n",
    "\n",
    "            cluster_id = [msms['cluster_id'] for msms in top_msms]\n",
    "            feature_id = [msms['feature_id'] for msms in top_msms]\n",
    "            dot_product_list = [msms['dot_product'] for msms in top_msms]\n",
    "            entropy_similarity_list = [msms['entropy_similarity'] for msms in top_msms]\n",
    "            avg_dot_product = np.mean(dot_product_list)\n",
    "            avg_entropy_similarity = np.mean(entropy_similarity_list)\n",
    "            avg_retention_time = np.mean([msms['ms2_retention_time'] for msms in top_msms])\n",
    "\n",
    "            # Merge centroids from top MS/MS\n",
    "            all_cent_ms2_peaks = [np.array(pairs) for msms in top_msms for pairs in [msms['cent_mz_intensity_pairs']]]\n",
    "            combined_cent_peaks = np.concatenate(all_cent_ms2_peaks, axis=0)\n",
    "            sorted_arr = combined_cent_peaks[combined_cent_peaks[:, 0].argsort()]\n",
    "\n",
    "            groups = []\n",
    "            current_group = [sorted_arr[0]]\n",
    "            for row in sorted_arr[0:]:\n",
    "                if abs(row[0] - current_group[1][0]) <= mz_merge_thresh:\n",
    "                    current_group.append(row)\n",
    "                else:\n",
    "                    groups.append(np.array(current_group))\n",
    "                    current_group = [row]\n",
    "            groups.append(np.array(current_group))  # Add last group\n",
    "\n",
    "            averaged_results = []\n",
    "            for group in groups:\n",
    "                avg_intensity = np.mean(group[:, 1])\n",
    "                avg_mz = np.mean(group[:, 0])\n",
    "                averaged_results.append([avg_mz, avg_intensity])\n",
    "            avg_ms2_cent_peaks = np.array(averaged_results)\n",
    "\n",
    "            results.append({\n",
    "                'cluster_id' : cluster_id,\n",
    "                'feature_id' : feature_id,\n",
    "                'peak_id': peak_id,\n",
    "                'dot_product_list': dot_product_list,\n",
    "                'avg_dot_product': avg_dot_product,\n",
    "                'avg_ms2_retention_time': avg_retention_time,\n",
    "                'num_msms': len(msms_list), # changed \n",
    "                'entropy_similarity_list': entropy_similarity_list,\n",
    "                'avg_entropy_similarity': avg_entropy_similarity,\n",
    "                'avg_ms2_cent_peaks': avg_ms2_cent_peaks.tolist()\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd6aa09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import ms_entropy as me\n",
    "# Version3 (chatgpt)\n",
    "def dot_product_with_tolerance(mz1, int1, mz2, int2, tol=0.02):\n",
    "    matched1, matched2 = [], []\n",
    "    for i, m1 in enumerate(mz1):\n",
    "        for j, m2 in enumerate(mz2):\n",
    "            if abs(m1 - m2) <= tol:\n",
    "                matched1.append(int1[i])\n",
    "                matched2.append(int2[j])\n",
    "                break\n",
    "    if not matched1:\n",
    "        return 0.0\n",
    "    s1 = np.array(matched1); s2 = np.array(matched2)\n",
    "    if np.linalg.norm(s1) == 0 or np.linalg.norm(s2) == 0:\n",
    "        return 0.0\n",
    "    s1 /= np.linalg.norm(s1); s2 /= np.linalg.norm(s2)\n",
    "    return float(np.dot(s1, s2))\n",
    "\n",
    "def process_spectral_similarity(df, top_n=3, mz_tolerance=0.02, mz_merge_thresh=0.01):\n",
    "    required = ['peak_id','cluster_id','file_id','feature_id','msms_id',\n",
    "                'ms2_rt','charge_state','ms2_peaks','cent_mz','cent_intensity']\n",
    "    assert all(c in df.columns for c in required), \"Missing required columns\"\n",
    "\n",
    "    results = []\n",
    "    for peak_id, group in df.groupby('peak_id'):\n",
    "        if len(group) < 2:\n",
    "            row = group.iloc[0]\n",
    "            results.append({\n",
    "                'cluster_id': row['cluster_id'],\n",
    "                'feature_id': row['feature_id'],\n",
    "                'peak_id': peak_id,\n",
    "                'avg_ms2_retention_time': row['ms2_rt'],\n",
    "                'num_msms': 1,\n",
    "                'charge_state': row['charge_state'],\n",
    "                'avg_ms2_cent_peaks': list(zip(row['cent_mz'], row['cent_intensity'])),\n",
    "                'raw_ms2_sample_peaks': row['ms2_peaks']\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        msms_list = []\n",
    "        for (_, r1), (_, r2) in combinations(group.iterrows(), 2):\n",
    "            dp = dot_product_with_tolerance(r1['cent_mz'], r1['cent_intensity'],\n",
    "                                            r2['cent_mz'], r2['cent_intensity'],\n",
    "                                            tol=mz_tolerance)\n",
    "            try:\n",
    "                ent = me.calculate_entropy_similarity(r1['ms2_peaks'], r2['ms2_peaks'])\n",
    "            except Exception:\n",
    "                ent = None\n",
    "            msms_list.append({\n",
    "                'cluster_id': r1['cluster_id'],\n",
    "                'feature_id': r1['feature_id'],\n",
    "                'dot_product': dp,\n",
    "                'entropy_similarity': ent,\n",
    "                'ms2_rt': r1['ms2_rt'],\n",
    "                'cent_pairs': list(zip(r1['cent_mz'], r1['cent_intensity']))\n",
    "            })\n",
    "\n",
    "        # filter out missing entropies\n",
    "        msms_list = [m for m in msms_list if m['entropy_similarity'] is not None]\n",
    "        if not msms_list:\n",
    "            continue\n",
    "\n",
    "        msms_list.sort(key=lambda x: x['entropy_similarity'], reverse=True)\n",
    "        top_msms = msms_list[:top_n]\n",
    "\n",
    "        cp = np.concatenate([np.array(m['cent_pairs']) for m in top_msms], axis=0)\n",
    "        cp = cp[cp[:,0].argsort()]\n",
    "\n",
    "        groups_c = []\n",
    "        current = [cp[0]]\n",
    "        for mz_i, intensity_i in cp[1:]:\n",
    "            if abs(mz_i - current[-1][0]) <= mz_merge_thresh:\n",
    "                current.append([mz_i, intensity_i])\n",
    "            else:\n",
    "                groups_c.append(np.array(current))\n",
    "                current = [[mz_i, intensity_i]]\n",
    "        groups_c.append(np.array(current))\n",
    "\n",
    "        avg_peaks = [[g[:,0].mean(), g[:,1].mean()] for g in groups_c]\n",
    "\n",
    "        results.append({\n",
    "            'cluster_id': [m['cluster_id'] for m in top_msms],\n",
    "            'feature_id': [m['feature_id'] for m in top_msms],\n",
    "            'peak_id': peak_id,\n",
    "            'avg_retention_time': np.mean([m['ms2_rt'] for m in top_msms]),\n",
    "            'num_msms': len(top_msms),\n",
    "            'dot_product_list': [m['dot_product'] for m in top_msms],\n",
    "            'avg_dot_product': np.mean([m['dot_product'] for m in top_msms]),\n",
    "            'entropy_similarity_list': [m['entropy_similarity'] for m in top_msms],\n",
    "            'avg_entropy_similarity': np.mean([m['entropy_similarity'] for m in top_msms]),\n",
    "            'avg_ms2_cent_peaks': avg_peaks\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import ms_entropy as me\n",
    "# Version3 - fixed\n",
    "def dot_product_with_tolerance(mz1, int1, mz2, int2, tol=0.02):\n",
    "    matched1, matched2 = [], []\n",
    "    for i, m1 in enumerate(mz1):\n",
    "        for j, m2 in enumerate(mz2):\n",
    "            if abs(m1 - m2) <= tol:\n",
    "                matched1.append(int1[i])\n",
    "                matched2.append(int2[j])\n",
    "                break\n",
    "    if not matched1:\n",
    "        return 0.0\n",
    "    s1 = np.array(matched1); s2 = np.array(matched2)\n",
    "    if np.linalg.norm(s1) == 0 or np.linalg.norm(s2) == 0:\n",
    "        return 0.0\n",
    "    s1 /= np.linalg.norm(s1); s2 /= np.linalg.norm(s2)\n",
    "    return float(np.dot(s1, s2))\n",
    "\n",
    "def process_spectral_similarity(df, top_n=3, mz_tolerance=0.02, mz_merge_thresh=0.01):\n",
    "    required = ['peak_id','cluster_id','file_id','feature_id','msms_id',\n",
    "                'ms2_rt','charge_state','ms2_peaks','cent_mz','cent_intensity']\n",
    "    assert all(c in df.columns for c in required), \"Missing required columns\"\n",
    "\n",
    "    results = []\n",
    "    for peak_id, group in df.groupby('peak_id'):\n",
    "        if len(group) < 2:\n",
    "            row = group.iloc[0]\n",
    "            results.append({\n",
    "                'cluster_id': row['cluster_id'],\n",
    "                'feature_id': row['feature_id'],\n",
    "                'peak_id': peak_id,\n",
    "                'avg_ms2_retention_time': row['ms2_rt'],\n",
    "                'num_msms': 1,\n",
    "                'charge_state': row['charge_state'],\n",
    "                'avg_ms2_cent_peaks': list(zip(row['cent_mz'], row['cent_intensity'])),\n",
    "                'raw_ms2_sample_peaks': row['ms2_peaks']\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        msms_list = []\n",
    "        for (_, r1), (_, r2) in combinations(group.iterrows(), 2):\n",
    "            dot_product = dot_product_with_tolerance(r1['cent_mz'], r1['cent_intensity'],\n",
    "                                            r2['cent_mz'], r2['cent_intensity'],\n",
    "                                            tol=mz_tolerance)\n",
    "            try:\n",
    "                entropy_similarity = me.calculate_entropy_similarity(r1['ms2_peaks'], r2['ms2_peaks'])\n",
    "            except Exception:\n",
    "                entropy_similarity = None\n",
    "            msms_list.append({\n",
    "                'cluster_id': r1['cluster_id'],\n",
    "                'feature_id': r1['feature_id'],\n",
    "                'dot_product': dot_product,\n",
    "                'entropy_similarity': entropy_similarity,\n",
    "                'ms2_rt': r1['ms2_rt'],\n",
    "                'cent_pairs': list(zip(r1['cent_mz'], r1['cent_intensity']))\n",
    "            })\n",
    "\n",
    "        # filter out missing entropies\n",
    "        msms_list = [m for m in msms_list if m['entropy_similarity'] is not None]\n",
    "        if not msms_list:\n",
    "            continue\n",
    "\n",
    "        msms_list.sort(key=lambda x: x['entropy_similarity'], reverse=True)\n",
    "        top_msms = msms_list[:top_n]\n",
    "\n",
    "        all_cent_peaks = np.concatenate([np.array(m['cent_pairs']) for m in top_msms], axis=0)\n",
    "        sorted_all_cent_peaks = all_cent_peaks[all_cent_peaks[:,0].argsort()]\n",
    "\n",
    "        groups_current = []\n",
    "        current = [sorted_all_cent_peaks[0]]\n",
    "        for mz_i, intensity_i in sorted_all_cent_peaks[1:]:\n",
    "            if abs(mz_i - current[-1][0]) <= mz_merge_thresh:\n",
    "                current.append([mz_i, intensity_i])\n",
    "            else:\n",
    "                groups_current.append(np.array(current))\n",
    "                current = [[mz_i, intensity_i]]\n",
    "        groups_current.append(np.array(current))\n",
    "\n",
    "        avg_cent_peaks = [[g[:,0].mean(), g[:,1].mean()] for g in groups_current]\n",
    "\n",
    "        results.append({\n",
    "            'cluster_id': [m['cluster_id'] for m in top_msms],\n",
    "            'feature_id': [m['feature_id'] for m in top_msms],\n",
    "            'peak_id': peak_id,\n",
    "            'avg_retention_time': np.mean([m['ms2_rt'] for m in top_msms]),\n",
    "            'total_num_msms': len(msms_list), # changed from (top_msms)\n",
    "            'dot_product_list': [m['dot_product'] for m in top_msms],\n",
    "            'avg_dot_product': np.mean([m['dot_product'] for m in top_msms]),\n",
    "            'entropy_similarity_list': [m['entropy_similarity'] for m in top_msms],\n",
    "            'avg_entropy_similarity': np.mean([m['entropy_similarity'] for m in top_msms]),\n",
    "            'avg_ms2_cent_peaks': avg_cent_peaks\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75187e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import ms_entropy as me\n",
    "# Version 4 - adding grouping by peak AND cluster - not working correctly \n",
    "def dot_product_with_tolerance(mz1, int1, mz2, int2, tol=0.02):\n",
    "    matched1, matched2 = [], []\n",
    "    for i, m1 in enumerate(mz1):\n",
    "        for j, m2 in enumerate(mz2):\n",
    "            if abs(m1 - m2) <= tol:\n",
    "                matched1.append(int1[i])\n",
    "                matched2.append(int2[j])\n",
    "                break\n",
    "    if not matched1:\n",
    "        return 0.0\n",
    "    s1 = np.array(matched1); s2 = np.array(matched2)\n",
    "    if np.linalg.norm(s1) == 0 or np.linalg.norm(s2) == 0:\n",
    "        return 0.0\n",
    "    s1 /= np.linalg.norm(s1); s2 /= np.linalg.norm(s2)\n",
    "    return float(np.dot(s1, s2))\n",
    "\n",
    "def process_spectral_similarity(df, top_n=3, mz_tolerance=0.02, mz_merge_thresh=0.01):\n",
    "    required = ['peak_id','cluster_id','file_id','feature_id','msms_id',\n",
    "                'ms2_rt','charge_state','ms2_peaks','cent_mz','cent_intensity']\n",
    "    assert all(c in df.columns for c in required), \"Missing required columns\"\n",
    "\n",
    "    results = []\n",
    "    for peak_id, group in df.groupby('peak_id'):\n",
    "        if len(group) < 2:\n",
    "            row = group.iloc[0]\n",
    "            results.append({\n",
    "                'cluster_id': row['cluster_id'],\n",
    "                'feature_id': row['feature_id'],\n",
    "                'peak_id': peak_id,\n",
    "                'avg_ms2_retention_time': row['ms2_rt'],\n",
    "                'num_msms': 1,\n",
    "                'charge_state': row['charge_state'],\n",
    "                'avg_ms2_cent_peaks': list(zip(row['cent_mz'], row['cent_intensity'])),\n",
    "                'raw_ms2_sample_peaks': row['ms2_peaks']\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        msms_list = []\n",
    "        for (_, r1), (_, r2) in combinations(group.iterrows(), 2):\n",
    "            if r1['cluster_id'] != r1['cluster_id']:\n",
    "                continue\n",
    "            else:\n",
    "                dot_product = dot_product_with_tolerance(r1['cent_mz'], r1['cent_intensity'],\n",
    "                                            r2['cent_mz'], r2['cent_intensity'],\n",
    "                                            tol=mz_tolerance)\n",
    "                try:\n",
    "                    entropy_similarity = me.calculate_entropy_similarity(r1['ms2_peaks'], r2['ms2_peaks'])\n",
    "                except Exception:\n",
    "                    entropy_similarity = None\n",
    "                msms_list.append({\n",
    "                    'cluster_id': r1['cluster_id'],\n",
    "                    'feature_id': r1['feature_id'],\n",
    "                    'dot_product': dot_product,\n",
    "                    'entropy_similarity': entropy_similarity,\n",
    "                    'ms2_rt': r1['ms2_rt'],\n",
    "                    'cent_pairs': list(zip(r1['cent_mz'], r1['cent_intensity']))\n",
    "                    })\n",
    "\n",
    "        # filter out missing entropies\n",
    "        msms_list = [m for m in msms_list if m['entropy_similarity'] is not None]\n",
    "        if not msms_list:\n",
    "            continue\n",
    "\n",
    "        msms_list.sort(key=lambda x: x['entropy_similarity'], reverse=True)\n",
    "        top_msms = msms_list[:top_n]\n",
    "\n",
    "        all_cent_peaks = np.concatenate([np.array(m['cent_pairs']) for m in top_msms], axis=0)\n",
    "        sorted_all_cent_peaks = all_cent_peaks[all_cent_peaks[:,0].argsort()]\n",
    "\n",
    "        groups_current = []\n",
    "        current = [sorted_all_cent_peaks[0]]\n",
    "        for mz_i, intensity_i in sorted_all_cent_peaks[1:]:\n",
    "            if abs(mz_i - current[-1][0]) <= mz_merge_thresh:\n",
    "                current.append([mz_i, intensity_i])\n",
    "            else:\n",
    "                groups_current.append(np.array(current))\n",
    "                current = [[mz_i, intensity_i]]\n",
    "        groups_current.append(np.array(current))\n",
    "\n",
    "        avg_cent_peaks = [[g[:,0].mean(), g[:,1].mean()] for g in groups_current]\n",
    "\n",
    "        results.append({\n",
    "            'cluster_id': [m['cluster_id'] for m in top_msms],\n",
    "            'feature_id': [m['feature_id'] for m in top_msms],\n",
    "            'peak_id': peak_id,\n",
    "            'avg_retention_time': np.mean([m['ms2_rt'] for m in top_msms]),\n",
    "            'total_num_msms': len(msms_list), # changed from (top_msms)\n",
    "            'dot_product_list': [m['dot_product'] for m in top_msms],\n",
    "            'avg_dot_product': np.mean([m['dot_product'] for m in top_msms]),\n",
    "            'entropy_similarity_list': [m['entropy_similarity'] for m in top_msms],\n",
    "            'avg_entropy_similarity': np.mean([m['entropy_similarity'] for m in top_msms]),\n",
    "            'avg_ms2_cent_peaks': avg_cent_peaks\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9663da17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cluster_id feature_id  peak_id  avg_retention_time  total_num_msms  \\\n",
      "0  [0, 0, 0]  [0, 0, 0]        0          418.381333           276.0   \n",
      "1  [1, 1, 1]  [6, 6, 4]        1          121.158667           300.0   \n",
      "2  [3, 2, 3]  [1, 1, 1]        2          530.039667           406.0   \n",
      "3  [3, 3, 3]  [1, 1, 1]        3          514.468667           253.0   \n",
      "4  [4, 4, 4]  [7, 7, 7]        4          325.467333           435.0   \n",
      "\n",
      "                                    dot_product_list  avg_dot_product  \\\n",
      "0  [0.999981201933493, 0.9999569268073769, 0.9999...         0.999970   \n",
      "1  [0.9999890733667712, 0.9999642718845316, 0.999...         0.999980   \n",
      "2  [0.9998662751866915, 0.999898686106641, 0.9999...         0.999904   \n",
      "3  [0.999883362650618, 0.9999969005307231, 0.9999...         0.999955   \n",
      "4  [0.9999772905900919, 0.999882199491073, 0.9998...         0.999904   \n",
      "\n",
      "                             entropy_similarity_list  avg_entropy_similarity  \\\n",
      "0  [0.9999998807907104, 0.9999997019767761, 0.999...                1.000000   \n",
      "1  [0.9999985694885254, 0.9999897480010986, 0.999...                0.999992   \n",
      "2  [0.9999999403953552, 0.9999992251396179, 0.999...                0.999999   \n",
      "3  [0.9999982118606567, 0.9999961853027344, 0.999...                0.999994   \n",
      "4  [0.9999951720237732, 0.9999856948852539, 0.999...                0.999988   \n",
      "\n",
      "                                  avg_ms2_cent_peaks  avg_ms2_retention_time  \\\n",
      "0  [[184.07427469889322, 1219044.0], [758.5382385...                     NaN   \n",
      "1  [[60.08061218261719, 120914.0], [86.0963706970...                     NaN   \n",
      "2  [[184.07738749186197, 3395693.3333333335], [52...                     NaN   \n",
      "3  [[86.09654235839844, 170170.0], [124.999797821...                     NaN   \n",
      "4  [[105.0, 101339.33333333333], [273.18769327799...                     NaN   \n",
      "\n",
      "   num_msms  charge_state raw_ms2_sample_peaks  \n",
      "0       NaN           NaN                  NaN  \n",
      "1       NaN           NaN                  NaN  \n",
      "2       NaN           NaN                  NaN  \n",
      "3       NaN           NaN                  NaN  \n",
      "4       NaN           NaN                  NaN  \n"
     ]
    }
   ],
   "source": [
    "df_results = process_spectral_similarity(df, top_n=3, mz_tolerance=0.02, mz_merge_thresh=0.01)\n",
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2bf5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  \n",
    "filepath = Path('CID_metadata/CID_CORRECT/spectral_similarity-FIXED2.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df_results.to_csv(Path('CID_metadata/CID_CORRECT/spectral_similarity-FIXED2.csv', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971f7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502de502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8af166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cent_int1 = [10899.0, 15120.0, 531285.0, 15893.0]\n",
    "norm_cent_int1 = cent_int1 / np.linalg.norm(cent_int1)\n",
    "cent_mz1 = [184.07272338867188, 758.5419921875, 758.5665283203125, 758.5889892578125]\n",
    "\n",
    "cent_int2 = [155793.0, 633537.0, 57330.0, 336513.0, 96078.0, 5272051.0]\n",
    "norm_cent_int2 = cent_int2 / np.linalg.norm(cent_int2)\n",
    "cent_mz2 = [60.08081817626953, 86.09672546386719, 104.10718536376953, 124.99991607666016, 166.06260681152344, 184.083251953125]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0cd238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = list(zip(norm_cent_int2, cent_mz2))\n",
    "ref = list(zip(norm_cent_int1, cent_mz1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38f2abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37100595235824585\n"
     ]
    }
   ],
   "source": [
    "import ms_entropy as me\n",
    "entropy_sim = me.calculate_entropy_similarity(ref, query, clean_spectra = False)\n",
    "print(entropy_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PASTAQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
