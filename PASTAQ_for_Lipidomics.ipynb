{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c90b61ec",
   "metadata": {},
   "source": [
    "1) Import the required modules and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c0817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pastaq as pq\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.colors as colors\n",
    "from collections import defaultdict\n",
    "import ms_entropy as me\n",
    "from pathlib import Path\n",
    "import re\n",
    "from itertools import combinations\n",
    "import os\n",
    "import ms_entropy as me\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9e814",
   "metadata": {},
   "source": [
    "2) Import the helper functions for PASTAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14652227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for PASTAQ demonstration\n",
    "import math as math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helper functions\n",
    "# Function to calculate the Euclidean distance\n",
    "def euclidean_distance(peak1, peak2):\n",
    "    peak1x = peak1.fitted_rt\n",
    "    peak1y = peak1.fitted_mz\n",
    "    peak2x = peak2.fitted_rt + peak2.rt_delta\n",
    "    peak2y = peak2.fitted_mz\n",
    "    return math.sqrt(((peak1x - peak2x)/(peak1.fitted_sigma_rt + peak2.fitted_sigma_rt)) ** 2 + ((peak1y - peak2y)/(peak1.fitted_sigma_mz + peak2.fitted_sigma_mz)) ** 2)\n",
    "\n",
    "# Function to find the closest peak\n",
    "def find_closest_peak(reference_peak, peaks, mz_tolerance = 1, rt_tolerance = 1):\n",
    "    closest_peak = None\n",
    "    min_distance = float('inf')\n",
    "    \n",
    "    for peak in peaks:\n",
    "        distance = euclidean_distance(reference_peak, peak)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_peak = peak\n",
    "    \n",
    "    if reference_peak.fitted_mz - mz_tolerance <= closest_peak.fitted_mz <= reference_peak.fitted_mz + mz_tolerance and reference_peak.fitted_rt - rt_tolerance <= closest_peak.fitted_rt + closest_peak.rt_delta <= reference_peak.fitted_rt + rt_tolerance:\n",
    "        return closest_peak\n",
    "    else:\n",
    "        print(\"Reference peak not found in the peaks list.\")\n",
    "        return None\n",
    "\n",
    "# get segment indices for a value\n",
    "def find_segments_indices(values, starts, ends):\n",
    "    \"\"\"\n",
    "    Finds the segment index for each value in the list.\n",
    "    \n",
    "    Args:\n",
    "        values: A list of values to find the segments for.\n",
    "        starts: A list of segment start values.\n",
    "        ends: A list of segment end values.\n",
    "    \n",
    "    Returns:\n",
    "        A list of indices corresponding to the segment each value lies in, or -1 if a value is not in any segment.\n",
    "    \"\"\"\n",
    "    # Ensure the lists are of the same length\n",
    "    if len(starts) != len(ends):\n",
    "        raise ValueError(\"The 'starts' and 'ends' lists must have the same length.\")\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    for value in values:\n",
    "        found = False\n",
    "        for i in range(len(starts)):\n",
    "            if starts[i] <= value <= ends[i]:\n",
    "                indices.append(i)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            indices.append(-1)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "# interpolate retention time to warped retention time using aligned segments limits\n",
    "def interpolate_values(values, starts, ends, new_starts, new_ends):\n",
    "    \"\"\"\n",
    "    Interpolates the input values based on corresponding segment index values.\n",
    "    \n",
    "    Args:\n",
    "        values: A list of values to interpolate.\n",
    "        starts: A list of segment start values.\n",
    "        ends: A list of segment end values.\n",
    "        new_starts: A list of new segment start values for interpolation.\n",
    "        new_ends: A list of new segment end values for interpolation.\n",
    "    \n",
    "    Returns:\n",
    "        A list of interpolated values.\n",
    "    \"\"\"\n",
    "    indices = find_segments_indices(values, starts, ends)\n",
    "    interpolated_values = []\n",
    "    \n",
    "    for value, index in zip(values, indices):\n",
    "        if index == -1:\n",
    "            interpolated_values.append(None)  # Value is not in any segment\n",
    "        else:\n",
    "            # Perform linear interpolation\n",
    "            old_start = starts[index]\n",
    "            old_end = ends[index]\n",
    "            new_start = new_starts[index]\n",
    "            new_end = new_ends[index]\n",
    "            \n",
    "            # Linear interpolation formula\n",
    "            if old_end != old_start:  # Avoid division by zero\n",
    "                interpolated_value = new_start + ((value - old_start) / (old_end - old_start)) * (new_end - new_start)\n",
    "            else:\n",
    "                interpolated_value = new_start  # If old_start == old_end, use new_start\n",
    "            \n",
    "            interpolated_values.append(interpolated_value)\n",
    "    \n",
    "    return interpolated_values\n",
    "\n",
    "# calculate Gaussian peak with mean, sigma, and height\n",
    "def gaussian(x, mean, sigma, height):\n",
    "    return height * np.exp(-((x - mean) ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "# plot Gaussian peak\n",
    "def plot_gaussian(mean, sigma, height):\n",
    "    x = np.linspace(mean - 4 * sigma, mean + 4 * sigma, 1000)\n",
    "    y = gaussian(x, mean, sigma, height)\n",
    "    plt.plot(x, y, label='fitted Gaussian peak', color='orange', alpha=0.75, linestyle='-', linewidth=1)\n",
    "\n",
    "# plot mass spectra considering that only non-eros values are included in the spectra\n",
    "def plot_msSpectra(mz, intensity, norm_mz_diff = 0.0035, diffFactor = 1.3, scanIdx = 0):\n",
    "    \"\"\"\n",
    "    Plotting mass spectra, expecting spectra where 0 intensity values were omitted.\n",
    "    \n",
    "    Args:\n",
    "        mz: A list of mz values of the mass spectra.\n",
    "        intensity: A list of intensity (non-zero) values of the mass spectra.\n",
    "        norm_mz_diff: Difference between two adjacent mz at measurement point corresponding to the original sampling frequency of the mass spectra.\n",
    "        diffFactor: Tolerance factor allowing to vary sampling frequency of mass spectra. It is typically set to 30% (factor 1.3).\n",
    "    \n",
    "    Returns:\n",
    "        Figure object as fig.\n",
    "    \"\"\"\n",
    "    spectra = {'mz': mz, 'intensity': intensity}\n",
    "    newSpectra = {'mz': [], 'intensity': []}\n",
    "    idxSpectra = 0\n",
    "    for i in range(1, len(spectra['mz'])):\n",
    "        diff = spectra['mz'][i]-spectra['mz'][i-1]\n",
    "        if diff > norm_mz_diff*diffFactor:\n",
    "            if diff < norm_mz_diff*2:\n",
    "                newSpectra['mz'].insert(idxSpectra, spectra['mz'][i-1] + diff/2)\n",
    "                newSpectra['intensity'].insert(idxSpectra, 0)\n",
    "                idxSpectra += 1\n",
    "                print(norm_mz_diff*diffFactor, diff, norm_mz_diff*diffFactor + diff, norm_mz_diff)\n",
    "                newSpectra['mz'].insert(idxSpectra, spectra['mz'][i])\n",
    "                newSpectra['intensity'].insert(idxSpectra, spectra['intensity'][i])\n",
    "                idxSpectra += 1\n",
    "            else:\n",
    "                newSpectra['mz'].insert(idxSpectra, spectra['mz'][i-1] + norm_mz_diff)\n",
    "                newSpectra['intensity'].insert(idxSpectra, 0)\n",
    "                idxSpectra += 1\n",
    "                newSpectra['mz'].insert(idxSpectra, spectra['mz'][i] - norm_mz_diff)\n",
    "                newSpectra['intensity'].insert(idxSpectra, 0)\n",
    "                idxSpectra += 1\n",
    "                newSpectra['mz'].insert(idxSpectra, spectra['mz'][i])\n",
    "                newSpectra['intensity'].insert(idxSpectra, spectra['intensity'][i])\n",
    "                idxSpectra += 1\n",
    "        else:\n",
    "            newSpectra['mz'].insert(idxSpectra, spectra['mz'][i])\n",
    "            newSpectra['intensity'].insert(idxSpectra, spectra['intensity'][i])\n",
    "            if (diff/diffFactor) > norm_mz_diff:\n",
    "                norm_mz_diff = diff\n",
    "            idxSpectra += 1\n",
    "\n",
    "    fig = plt.figure(figsize=(25, 6), facecolor='white')  # Set the figure size and white background\n",
    "    plt.plot(newSpectra['mz'], newSpectra['intensity'], color = 'red', marker='', linestyle='-')  # Plot mz vs. intensity\n",
    "    plt.xlabel('m/z')  # Set the x-axis label\n",
    "    plt.ylabel('Intensity')  # Set the y-axis label\n",
    "    plt.title('Mass Spectrum of scan {}' .format(scanIdx))  # Set the title\n",
    "    plt.grid(False)  # Show grid\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200c874",
   "metadata": {},
   "source": [
    "3) Define the parameters (the parameter file used here is labelled as \"TOF_parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224402a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pastaq_parameters = pq.TOF_parameters(instrument='TOF', avg_fwhm_rt=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae515553",
   "metadata": {},
   "source": [
    "4) Input the files in the format of .mzML or .mzXML. The user can either select one sample to be used as the reference or they can set reference to 'false' for all samples and PASTAQ will determine which sample should be used as the reference. NB: this pipeline was run using a modified version of the PASTAQ __init__ py, it may be necessary to change 1350 to 1363 in the __init__ py file for PASTAQ version 0.11.2 to the lines of code provided here in the .py file labelled \"Change_to_features_PQ\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7173d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\n",
    "    {'reference': False, 'raw_path': r'C:\\Users\\Folder\\Subfolder\\sampleX1.mzXML', 'group': 'a'},\n",
    "    {'reference': False, 'raw_path': r'C:\\Users\\Folder\\Subfolder\\sampleX2.mzXML', 'group': 'a'},\n",
    "    {'reference': False, 'raw_path': r'C:\\Users\\Folder\\Subfolder\\sampleX3.mzXML', 'group': 'a'},\n",
    "    {'reference': False, 'raw_path': r'C:\\Users\\Folder\\Subfolder\\sampleY1.mzXML', 'group': 'b'},\n",
    "    {'reference': False, 'raw_path': r'C:\\Users\\Folder\\Subfolder\\sampleY2.mzXML', 'group': 'b'},\n",
    "    {'reference': False, 'raw_path': r'C:\\Users\\Folder\\Subfolder\\sampleY3.mzXML', 'group': 'b'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186199f7",
   "metadata": {},
   "source": [
    "5) DDA_Pipeline()\n",
    "Run the pipeline. Force override will overwrite any files that are already in the output directory. Changing \"save_grid\" to true might be useful if you want to look at your samples as graphs/ EICs etc, but I think it might take a very long time to run if you have a lot of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dda_pipeline = pq.dda_pipeline(\n",
    "    pastaq_parameters,\n",
    "    input_files,\n",
    "    output_dir=\"pastaq\",\n",
    "    force_override=False,\n",
    "    save_grid=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007e408",
   "metadata": {},
   "source": [
    "2) Import the MS2 information (.ms2 files) output from PASTAQ's DDA pipeline (these will be in the folder called \"raw\"). Fill in the path to these files as they appear on your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [{'raw_path': r\"C:\\Users\\pastaq\\raw\\sampleX1.ms2\"},\n",
    "               {'raw_path': r\"C:\\Users\\pastaq\\raw\\sampleX2.ms2\"},\n",
    "               {'raw_path': r\"C:\\Users\\pastaq\\raw\\sampleX3.ms2\"},\n",
    "               {'raw_path': r\"C:\\Users\\pastaq\\raw\\sampleY1.ms2\"},\n",
    "               {'raw_path': r\"C:\\Users\\pastaq\\raw\\sampleY2.ms2\"},\n",
    "               {'raw_path': r\"C:\\Users\\pastaq\\raw\\sampleY3.ms2\"}\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"C:\\Users\\pastaq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321096f0",
   "metadata": {},
   "source": [
    "3) Import the csv file called \"feature_clusters_annotations\" output from PASTAQ's CSV file (it will be in the folder called \"quant\"). It can be read using the read_csv function provided by PANDAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c61321",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_clusters_annotations_csv = pd.read_csv(r\"C:\\Users\\pastaq\\quant\\feature_clusters_annotations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee71293",
   "metadata": {},
   "source": [
    "4) combine_multiple_samples()\n",
    "Run the combine_multiple_samples() function to link the MS2 information to the csv file containing the feature clusters. This function will skip and therefore remove any features that have no linked MSMS. If you wish to keep these features that have no linked MSMS, the function will have to be manually adjusted. The 'stem' of the input file will be used to obtain the file name. The data is extracted using the pre-existing read_raw_data() function from PASTAQ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to link the MS2 information to the csv file containing the feature clusters, this function will skip and therefore remove any features that have no linked MSMS\n",
    "def combine_multiple_samples(feature_clusters_annotations_csv, input_files, output_dir):\n",
    "    # Preprocess: build a lookup dictionary to avoid filtering the DataFrame each time\n",
    "    annotations_lookup = defaultdict(list)\n",
    "\n",
    "    for _, row in feature_clusters_annotations_csv.iterrows():\n",
    "        if pd.notnull(row['msms_id']):\n",
    "            key = (row['file_id'], row['msms_id'])\n",
    "            annotations_lookup[key].append(row)\n",
    "\n",
    "    combined_multiple_samples = []\n",
    "\n",
    "    for file in input_files:\n",
    "        if 'stem' not in file:\n",
    "            base_name = os.path.splitext(os.path.basename(file['raw_path']))[0]\n",
    "            file['stem'] = base_name\n",
    "        stem = file['stem']\n",
    "        in_path = os.path.join(output_dir, 'raw', f\"{stem}.ms2\")\n",
    "\n",
    "        if not os.path.exists(in_path):\n",
    "            continue\n",
    "\n",
    "        raw_data = pq.read_raw_data(in_path)\n",
    "\n",
    "        for scan in raw_data.scans:\n",
    "            scan_number = scan.scan_number # The scan number is the same number as the msms_id  number \n",
    "            key = (stem, scan_number)\n",
    "            annotations = annotations_lookup.get(key)\n",
    "\n",
    "            if not annotations:\n",
    "                continue\n",
    "\n",
    "            ms2_mz = scan.mz\n",
    "            ms2_intensity = scan.intensity\n",
    "            ms2_rt = scan.retention_time\n",
    "\n",
    "            # Remove any scans that have no valid spectra (if necessary)\n",
    "            if not ms2_mz or not ms2_intensity or len(ms2_mz) != len(ms2_intensity):\n",
    "                continue\n",
    "\n",
    "            # Convert to numpy array for faster sorting\n",
    "            mz_array = np.array(ms2_mz)\n",
    "            intensity_array = np.array(ms2_intensity)\n",
    "            sorted_indices = np.argsort(mz_array)\n",
    "            mz_intensity_pairs = list(zip(mz_array[sorted_indices], intensity_array[sorted_indices]))\n",
    "            ms2_peaks = np.array(mz_intensity_pairs, dtype=np.float32)\n",
    "\n",
    "            for row in annotations:\n",
    "                combined_multiple_samples.append({\n",
    "                    'cluster_id': row['cluster_id'],\n",
    "                    'file_id': row['file_id'],\n",
    "                    'feature_id': row['feature_id'],\n",
    "                    'peak_id': row['peak_id'],\n",
    "                    'msms_id': row['msms_id'],\n",
    "                    'ms2_rt': ms2_rt,\n",
    "                    'charge_state': row['charge_state'],\n",
    "                    'ms2_peaks' : ms2_peaks\n",
    "                })\n",
    "\n",
    "\n",
    "    return combined_multiple_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97784fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_multiple_samples = combine_multiple_samples(feature_clusters_annotations_csv, input_files, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f010409",
   "metadata": {},
   "source": [
    "5) Input the files containing the feature information output from PASTAQ's DDA pipeline (.features files). They will be in the folder labelled \"features\". This may be redundant as I have accidentally run it without running the input files and the function was able to find and extract the .features files without specifying the input files. However, I have included the step here in case anyone runs into problems with leaving it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff25916",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [{'raw_path': r\"C:\\Users\\pastaq\\raw\\sampleX1.features\"},\n",
    "               {'raw_path': r\"C:\\Users\\pastaq\\raw\\sampleX2.features\"},\n",
    "               {'raw_path': r\"C:\\Users\\pastaq\\raw\\sampleX3.features\"},\n",
    "               {'raw_path': r\"C:\\Users\\pastaq\\raw\\sampleY1.features\"},\n",
    "               {'raw_path': r\"C:\\Users\\pastaq\\raw\\sampleY2.features\"},\n",
    "               {'raw_path': r\"C:\\Users\\pastaq\\raw\\sampleY3.features\"}\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ec1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"C:\\Users\\pastaq\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850526b",
   "metadata": {},
   "source": [
    "6) link_features()\n",
    "Run the function that links the features to the large dataframe now containing both the MS2 information and the feature cluster information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290870a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to link the MS1 features to the large dataframe now containing both the MS2 information and the feature cluster informations:\n",
    "def link_features(combined_multiple_samples, input_files, output_dir):\n",
    "    # Preprocess: build a lookup dictionary to avoid filtering the DataFrame each time\n",
    "    annotations_lookup = defaultdict(list)\n",
    "    linked_features = []\n",
    "\n",
    "    for item in combined_multiple_samples:\n",
    "        if pd.notnull(item['feature_id']):\n",
    "            key = (item['file_id'], item['feature_id'])\n",
    "            annotations_lookup[key].append(item)\n",
    "\n",
    "    for file in input_files:\n",
    "        if 'stem' not in file:\n",
    "            base_name = os.path.splitext(os.path.basename(file['raw_path']))[0]\n",
    "            file['stem'] = base_name\n",
    "        stem = file['stem']\n",
    "        in_path_features = os.path.join(output_dir, 'features', f\"{stem}.features\")\n",
    "\n",
    "        if not os.path.exists(in_path_features):\n",
    "            print('missing feature file/s')\n",
    "            continue\n",
    "\n",
    "        features = pq.read_features(in_path_features)\n",
    "\n",
    "        for feature in features:\n",
    "            id = feature.id\n",
    "            key = (stem, id)\n",
    "            annotations = annotations_lookup.get(key)\n",
    "\n",
    "            if not annotations:\n",
    "                continue\n",
    "\n",
    "            if isinstance(annotations, list):\n",
    "                for annotation in annotations:\n",
    "                    linked_features.append({\n",
    "                        'cluster_id': annotation['cluster_id'],\n",
    "                        'file_id': annotation['file_id'],\n",
    "                        'feature_id': id,\n",
    "                        'feature_peak_ids': feature.peak_ids,\n",
    "                        'peak_id' : annotation['peak_id'],\n",
    "                        'msms_id': annotation['msms_id'],\n",
    "                        'precursor_mz' : feature.monoisotopic_mz,\n",
    "                        'precursor_rt': feature.monoisotopic_rt,\n",
    "                        'precursor_intensity': feature.monoisotopic_height,\n",
    "                        'precursor_vol': feature.monoisotopic_volume,\n",
    "                        'total_intensity': feature.total_height,\n",
    "                        'total_volume': feature.total_volume,\n",
    "                        'average_ms1_mz': feature.average_mz,\n",
    "                        'average_rt': feature.average_rt,\n",
    "                        'charge_state': feature.charge_state,\n",
    "                        'ms2_sample_peaks': annotation['ms2_peaks'],                  \n",
    "                        })\n",
    "\n",
    "    return linked_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71768f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_features = link_features(combined_multiple_samples=combined_multiple_samples, input_files=input_files, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126b266",
   "metadata": {},
   "source": [
    "7) average_msms()\n",
    "This step is optional, please proceed to markdown box 11 if you do not wish to do this. Run the function that finds and averages out the top MSMS linked to one peak. Here \"top\" is defined by the highest entropy similarity score calculated by MS entropy. The merge threshhold is the threshold that determines when two MS2 values should be added together and averaged out. Top_n is a parameter that determines the number of top spectra to be considered. The code was developed using a top_n of 3 and a mz_merge_thresh of 0.01. The \"linked_features\" are first converted to a dataframe using PANDAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a23096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(linked_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the dot product for the function that finds the best average MS2 value for MS1 peaks with multiple linked MS2 values\n",
    "def dot_product_with_tolerance(mz1, int1, mz2, int2, tol=0.02):\n",
    "    matched1, matched2 = [], []\n",
    "    for i, m1 in enumerate(mz1):\n",
    "        for j, m2 in enumerate(mz2):\n",
    "            if abs(m1 - m2) <= tol:\n",
    "                matched1.append(int1[i])\n",
    "                matched2.append(int2[j])\n",
    "                break\n",
    "    if not matched1:\n",
    "        return 0.0\n",
    "    s1 = np.array(matched1); s2 = np.array(matched2)\n",
    "    if np.linalg.norm(s1) == 0 or np.linalg.norm(s2) == 0:\n",
    "        return 0.0\n",
    "    s1 /= np.linalg.norm(s1); s2 /= np.linalg.norm(s2)\n",
    "    return float(np.dot(s1, s2))\n",
    "\n",
    "# Function to find the best average MS2 value for MS1 peaks with multiple linked MS2 values\n",
    "def average_msms(df, top_n=3, mz_tolerance=0.02, mz_merge_thresh=0.01):\n",
    "    required = ['peak_id','cluster_id','file_id','feature_id','msms_id',\n",
    "                'ms2_rt','charge_state','ms2_sample_peaks','cent_mz','cent_intensity','precursor_mz',\n",
    "                'precursor_rt','precursor_intensity','precursor_vol','total_intensity','total_volume',\n",
    "                'average_ms1_mz','average_rt']\n",
    "\n",
    "    assert all(c in df.columns for c in required), \"Missing required columns\"\n",
    "\n",
    "    results = []\n",
    "    for (peak_id, file_id), group in df.groupby(['peak_id', 'file_id']):\n",
    "        if len(group) < 2:\n",
    "            row = group.iloc[0]\n",
    "            results.append({\n",
    "                'cluster_id': row['cluster_id'],\n",
    "                'file_id' : file_id,\n",
    "                'feature_id': row['feature_id'],\n",
    "                'peak_id': peak_id,\n",
    "                'avg_ms2_retention_time': row['ms2_rt'],\n",
    "                'total_num_msms': 1,\n",
    "                'charge_state': row['charge_state'],\n",
    "                'avg_ms2_cent_peaks': list(zip(row['cent_mz'], row['cent_intensity'])),\n",
    "                'avg_raw_ms2_sample_peaks': row['ms2_sample_peaks'],\n",
    "                'avg_precursor_mz': row['precursor_mz'],\n",
    "                'precursor_mz_list': row['precursor_mz'],\n",
    "                'avg_precursor_rt': row['precursor_rt'],\n",
    "                'precursor_rt_list': row['precursor_rt'],\n",
    "                'avg_precursor_intensity' : row['precursor_intensity'],\n",
    "                'precursor_intensity_list' : row['precursor_intensity'],\n",
    "                'avg_precursor_vol': row['precursor_vol'],\n",
    "                'precursor_vol_list': row['precursor_vol'],\n",
    "                'dot_product_list': [],\n",
    "                'avg_dot_product': 0.0,\n",
    "                'entropy_similarity_list': [],\n",
    "                'avg_entropy_similarity': 0.0,\n",
    "                'ms2_sample_peaks' : row['ms2_sample_peaks']\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        msms_list = []\n",
    "        for (_, r1), (_, r2) in combinations(group.iterrows(), 2):\n",
    "            dot_product = dot_product_with_tolerance(r1['cent_mz'], r1['cent_intensity'],\n",
    "                                            r2['cent_mz'], r2['cent_intensity'],\n",
    "                                            tol=mz_tolerance)\n",
    "            try:\n",
    "                entropy_similarity = me.calculate_entropy_similarity(r1['ms2_sample_peaks'], r2['ms2_sample_peaks'])\n",
    "            except Exception:\n",
    "                entropy_similarity = None\n",
    "            msms_list.append({\n",
    "                'cluster_id': r1['cluster_id'],\n",
    "                'feature_id': r1['feature_id'],\n",
    "                'dot_product': dot_product,\n",
    "                'entropy_similarity': entropy_similarity,\n",
    "                'ms2_rt': r1['ms2_rt'],\n",
    "                'cent_pairs': list(zip(r1['cent_mz'], r1['cent_intensity'])),\n",
    "                'ms2_sample_peaks' : r1['ms2_sample_peaks'],\n",
    "                'charge_state': r1['charge_state'],\n",
    "                'precursor_mz': r1['precursor_mz'],\n",
    "                'precursor_rt': r1['precursor_rt'],\n",
    "                'precursor_intensity' : r1['precursor_intensity'],\n",
    "                'precursor_vol': r1['precursor_vol'],\n",
    "            })\n",
    "\n",
    "        # filter out missing entropies\n",
    "        msms_list = [m for m in msms_list if m['entropy_similarity'] is not None]\n",
    "        if not msms_list:\n",
    "            continue\n",
    "\n",
    "        msms_list.sort(key=lambda x: x['entropy_similarity'], reverse=True)\n",
    "        top_msms = msms_list[:top_n]\n",
    "\n",
    "        all_cent_peaks = np.concatenate([np.array(m['cent_pairs']) for m in top_msms], axis=0)\n",
    "        sorted_all_cent_peaks = all_cent_peaks[all_cent_peaks[:,0].argsort()]\n",
    "\n",
    "        groups_current = []\n",
    "        current = [sorted_all_cent_peaks[0]]\n",
    "        for mz_i, intensity_i in sorted_all_cent_peaks[1:]:\n",
    "            if abs(mz_i - current[-1][0]) <= mz_merge_thresh:\n",
    "                current.append([mz_i, intensity_i])\n",
    "            else:\n",
    "                groups_current.append(np.array(current))\n",
    "                current = [[mz_i, intensity_i]]\n",
    "        groups_current.append(np.array(current))\n",
    "\n",
    "        avg_cent_peaks = [[g[:,0].mean(), g[:,1].mean()] for g in groups_current]\n",
    "        centroided_arr = np.array(avg_cent_peaks)\n",
    "        centroided_arr_list = centroided_arr.tolist()\n",
    "\n",
    "        all_raw_peaks = np.concatenate([np.array(m['ms2_sample_peaks']) for m in top_msms], axis=0)\n",
    "        sorted_all_raw_peaks = all_raw_peaks[all_raw_peaks[:,0].argsort()]\n",
    "\n",
    "        groups_current_raw = []\n",
    "        current_raw = [sorted_all_raw_peaks[0]]\n",
    "        for mz_i, intensity_i in sorted_all_cent_peaks[1:]:\n",
    "            if abs(mz_i - current[-1][0]) <= mz_merge_thresh:\n",
    "                current.append([mz_i, intensity_i])\n",
    "            else:\n",
    "                groups_current_raw.append(np.array(current_raw))\n",
    "                current_raw = [[mz_i, intensity_i]]\n",
    "        groups_current_raw.append(np.array(current_raw))\n",
    "\n",
    "        avg_raw_peaks = [[g[:,0].mean(), g[:,1].mean()] for g in groups_current_raw]\n",
    "        raw_arr = np.array(avg_raw_peaks)\n",
    "        raw_arr_list = raw_arr.tolist()\n",
    "\n",
    "        results.append({\n",
    "            'cluster_id': [m['cluster_id'] for m in top_msms],\n",
    "            'file_id' : file_id,\n",
    "            'feature_id': [m['feature_id'] for m in top_msms],\n",
    "            'peak_id': peak_id,\n",
    "            'avg_ms2_retention_time': np.mean([m['ms2_rt'] for m in top_msms]),\n",
    "            'total_num_msms': len(msms_list), # changed from (top_msms)\n",
    "            'dot_product_list': [m['dot_product'] for m in top_msms],\n",
    "            'avg_dot_product': np.mean([m['dot_product'] for m in top_msms]),\n",
    "            'entropy_similarity_list': [m['entropy_similarity'] for m in top_msms],\n",
    "            'avg_entropy_similarity': np.mean([m['entropy_similarity'] for m in top_msms]),\n",
    "            'avg_ms2_cent_peaks': centroided_arr_list,\n",
    "            'charge_state': [m['charge_state'] for m in top_msms],\n",
    "            'precursor_mz_list': [m['precursor_mz'] for m in top_msms],\n",
    "            'avg_precursor_mz' : np.mean([m['precursor_mz'] for m in top_msms]),\n",
    "            'precursor_rt_list': [m['precursor_rt'] for m in top_msms],\n",
    "            'avg_precursor_rt' : np.mean([m['precursor_rt'] for m in top_msms]),\n",
    "            'precursor_intensity_list': [m['precursor_intensity'] for m in top_msms],\n",
    "            'avg_precursor_intensity' : np.mean([m['precursor_intensity'] for m in top_msms]),\n",
    "            'precursor_vol_list': [m['precursor_vol'] for m in top_msms],\n",
    "            'avg_precursor_vol' : np.mean([m['precursor_vol'] for m in top_msms]),\n",
    "            'avg_raw_ms2_sample_peaks' : raw_arr_list\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a4bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = average_msms(df, top_n=3, mz_tolerance=0.02, mz_merge_thresh=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bbb2d1",
   "metadata": {},
   "source": [
    "8) read_msp_file()\n",
    "Define the path to and read your MSP file. The function to read the MSP file will automatically convert the retention time from minutes to seconds, if your MSP data is already given in seconds you will have to remove that line of code. This code will also remove any annotations with missing or invalid retention times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse any strings that may be present in the MSP file\n",
    "def parse_array_from_string(s):\n",
    "    if isinstance(s, str):\n",
    "        return np.array([float(x) for x in re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", s)])\n",
    "    return np.array([])\n",
    "\n",
    "# Function to read and retrieve the annotations from an MSP file in the .msp format\n",
    "def read_msp_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    spectra_data = []\n",
    "    current_spectrum = {}\n",
    "    peak_data_started = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"Num Peaks\"):\n",
    "            peak_data_started = True\n",
    "            continue\n",
    "\n",
    "        if not peak_data_started:\n",
    "            if line.startswith(\"NAME:\"):\n",
    "                if \"|\" in line:\n",
    "                    # If the line contains '|', split the line after the '|' character\n",
    "                    parts = line.split('|')\n",
    "                    current_spectrum['name'] = parts[0].replace(\"NAME:\", \"\").strip()  # Remove \"NAME:\" and strip any extra spaces\n",
    "                    current_spectrum['saturation'] = parts[1].strip()  # After the '|'\n",
    "                else:\n",
    "                    # Otherwise, just use the name if no saturation is specified\n",
    "                    current_spectrum['name'] = line.split(\":\", 1)[1].strip()\n",
    "               \n",
    "            elif line.startswith(\"PRECURSORMZ:\"):\n",
    "                current_spectrum['precursor_mz'] = float(line.split(\":\", 1)[1].strip())\n",
    "            elif line.startswith(\"PRECURSORTYPE:\"):\n",
    "                current_spectrum['precursor_type'] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"IONMODE:\"):\n",
    "                current_spectrum['ion_mode'] = line.split(\":\", 1)[1].strip()              \n",
    "            elif line.startswith(\"RETENTIONTIME:\"):\n",
    "                _, raw = line.split(\":\", 1)\n",
    "                val = raw.strip()\n",
    "                if not val:\n",
    "                    print(\"[SKIP] empty retention_time\")\n",
    "                    continue\n",
    "                try:\n",
    "                    rt = float(val)\n",
    "                except ValueError:\n",
    "                        rt_parsed = parse_array_from_string(val)\n",
    "                        if isinstance(rt_parsed, (list, np.ndarray)) and len(rt_parsed) == 1:\n",
    "                            rt = float(rt_parsed[0])\n",
    "                        else:\n",
    "                            print(\"[SKIP] array invalid, skipping\")\n",
    "                            continue\n",
    "                current_spectrum['retention_time'] = rt * 60 # convert to seconds\n",
    "            elif line.startswith(\"Name: \"):\n",
    "                if current_spectrum:\n",
    "                    spectra_data.append(current_spectrum)\n",
    "                    current_spectrum = {\"Name\": line.split(\":\",1)[1].strip()}\n",
    "            elif line.startswith(\"FORMULA:\"):\n",
    "                current_spectrum['formula'] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"INCHIKEY:\"):\n",
    "                current_spectrum['inchi_key'] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"SMILES:\"):\n",
    "                current_spectrum['smiles'] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"COMMENT:\"):\n",
    "                current_spectrum['comment'] = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                mz, intensity = map(float, line.split())\n",
    "                current_spectrum.setdefault('peaks', []).append((mz, intensity))\n",
    "            except ValueError:\n",
    "                # This is where the spectrum data is stored and new spectrum begins\n",
    "                if current_spectrum:\n",
    "                    spectra_data.append(current_spectrum)\n",
    "                current_spectrum = {}  # Reset for the next spectrum\n",
    "                peak_data_started = False  # Reset peak reading flag\n",
    "\n",
    "    # Add last spectrum if it exists\n",
    "    if current_spectrum:\n",
    "        spectra_data.append(current_spectrum)\n",
    "    \n",
    "    return spectra_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd188b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_file = r\"C:\\Users\\path_to_your_msp_file.msp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83960ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_data = read_msp_file(msp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb8278",
   "metadata": {},
   "source": [
    "9) link_msp() - the function is the link MSP function that is used if the peaks with multiple MSMS have been averaged out.\n",
    "Link the annotations from the MSP file to the dataframe containing all the information for your samples. The code was developed using an rt tolerance of 8 seconds and an m/z tolerance of 0.025. You can set the tolerance yourself, however, you may have to adjust the \"match_score\" calculations if you change the tolerance. This function only links annotations to features on the MS1 level, using the precursor rt and precursor m/z to determine a score that measures the weighted difference between the m/z and rt of the sample compared to the reference. This function performs quality checks on both the MS1 and MS2 level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b91641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATTENTION - This function is the link MSP function that is used if the peaks with multiple MSMS have been averaged out\n",
    "def link_msp(results, msp_data, mz_tolerance=0.025, rt_tolerance=8.0):\n",
    "    linked_msp_features = []\n",
    "\n",
    "    # Loop through a list of Feature objects\n",
    "    for feature in results:\n",
    "        cluster_id = feature['cluster_id']\n",
    "        feature_id = feature['feature_id']\n",
    "        peak_id = feature['peak_id']\n",
    "        avg_precursor_mz = feature['avg_precursor_mz']\n",
    "        avg_precursor_rt = feature['avg_precursor_rt']\n",
    "        avg_precursor_intensity = feature['avg_precursor_intensity']\n",
    "        avg_precursor_volume = feature['avg_precursor_vol']\n",
    "        avg_ms2_cent_peaks = feature['avg_ms2_cent_peaks']\n",
    "        avg_raw_ms2_sample_peaks = feature['avg_raw_ms2_sample_peaks']\n",
    "        charge_state = feature['charge_state']              \n",
    "        avg_normalized_area = avg_precursor_volume * 114.7977026\n",
    "\n",
    "        # Find all matching annotations for the current scan\n",
    "        best_match = None  # To store the best match found\n",
    "\n",
    "        for annotation in msp_data:\n",
    "            # Ensure required fields are present in the annotation\n",
    "            if 'precursor_mz' not in annotation or 'retention_time' not in annotation:\n",
    "                continue  # Skip this annotation\n",
    "\n",
    "            else:\n",
    "                # Calculate mz and rt distances directly for scalars\n",
    "                mz_distance = np.abs(avg_precursor_mz - annotation['precursor_mz']) # changed this from 'mz' to 'precursor_mz'\n",
    "                rt_distance = np.abs(avg_precursor_rt - annotation['retention_time'])  # changed this from 'retention_time' to 'precursor_rt' \n",
    "\n",
    "                # Apply the tolerance checks\n",
    "                if mz_distance <= mz_tolerance and rt_distance <= rt_tolerance:\n",
    "                    # If the distances are within tolerance, calculate the match score with normalization factor\n",
    "                    match_score = (rt_distance*0.025) + (mz_distance*20) # lower score is better/ closer match; added weighting\n",
    "                \n",
    "                    # Calculate mass error in ppm using formula\n",
    "                    mass_error_ppm = ((annotation['precursor_mz'] - avg_precursor_mz) / annotation['precursor_mz']) * 10**6\n",
    "                \n",
    "                    #Calculate root mean squared error for best match\n",
    "                    y_true_mz = [annotation.get('precursor_mz')]\n",
    "                    y_pred_mz = [avg_precursor_mz]\n",
    "                    rmse_mz = root_mean_squared_error(y_true_mz, y_pred_mz)\n",
    "\n",
    "                    # Convert peaks to numpy arrays for similarity calculation (MS_Entropy)\n",
    "                    peaks_query = np.array(avg_raw_ms2_sample_peaks, dtype=np.float32) #peaks from given samples\n",
    "                    peaks_reference = np.array(annotation.get('peaks'), dtype=np.float32)  # peaks from msp\n",
    "                    if peaks_reference.ndim == 3 and peaks_reference.shape[0] == 1:\n",
    "                        peaks_reference = peaks_reference[0]  # flatten if needed\n",
    "\n",
    "                    # Skip if still not 2D\n",
    "                    if peaks_reference.ndim != 2:\n",
    "                            continue\n",
    "                    \n",
    "                    # Calculate similarity scores (MS_entropy)\n",
    "                    unweighted_similarity = me.calculate_unweighted_entropy_similarity(peaks_query, peaks_reference)\n",
    "                    similarity = me.calculate_entropy_similarity(peaks_query, peaks_reference) # entropy based intensity weights are applied to the peaks\n",
    "\n",
    "                    # Extract m/z and intensity values\n",
    "                    reference_mz, reference_intensity = zip(*peaks_reference)\n",
    "\n",
    "                    # Convert to NumPy arrays\n",
    "                    # Parse the mz and intensity arrays\n",
    "                    sample_mz, sample_intensity = zip(*avg_ms2_cent_peaks)\n",
    "                    sample_mz = np.array(sample_mz)\n",
    "                    sample_intensity = np.array(sample_intensity, dtype=np.float32)\n",
    "                    reference_mz = np.array(reference_mz)\n",
    "                    reference_intensity = np.array(reference_intensity, dtype=np.float32)\n",
    "\n",
    "                    # Match peaks based on m/z values\n",
    "                    matched_indices = np.searchsorted(reference_mz, sample_mz)\n",
    "\n",
    "                    # Ensure indices are within bounds (removes any indices that are out of bounds)\n",
    "                    matched_indices = matched_indices[matched_indices < len(reference_mz)]\n",
    "\n",
    "                    # Example mass spectra intensities\n",
    "                    sample_spectrum = np.array(sample_intensity[:len(matched_indices)])\n",
    "                    reference_spectrum = np.array(reference_intensity[matched_indices])\n",
    "\n",
    "                    # Normalize both vectors to unit length (L2 norm)\n",
    "                    sample_spectrum_normalized = sample_spectrum / np.linalg.norm(sample_spectrum)\n",
    "                    reference_spectrum_normalized = reference_spectrum / np.linalg.norm(reference_spectrum)\n",
    "\n",
    "                    # Compute dot product (cosine similarity if normalized)\n",
    "                    if len(sample_spectrum) < 3 or len(reference_spectrum) < 3:\n",
    "                        dot_product = None\n",
    "                    else:\n",
    "                        dot_product = np.dot(sample_spectrum_normalized, reference_spectrum_normalized)\n",
    "                \n",
    "                    if best_match is None or match_score < best_match['score']:\n",
    "                        best_match = {\n",
    "                            'score': float(match_score),  # Store the match score\n",
    "                            'name': annotation.get('name', None),\n",
    "                            'saturation' : annotation.get('saturation', None),\n",
    "                            'retention_time': annotation.get('retention_time', None),\n",
    "                            'precursor_mz': annotation.get('precursor_mz', None),\n",
    "                            'precursor_type': annotation.get('precursor_type', None),\n",
    "                            'smiles': annotation.get('smiles', None),\n",
    "                            'msp_peaks': annotation.get('peaks', None),\n",
    "                            }\n",
    "        \n",
    "                        # Create the annotated scan with only the best match\n",
    "                        linked_msp_feature = {\n",
    "                            'cluster_id' : cluster_id,\n",
    "                            'feature_id': feature_id,\n",
    "                            'peak_id': peak_id,\n",
    "                            'avg_precursor_mz': avg_precursor_mz,\n",
    "                            'avg_precursor_intensity' : avg_precursor_intensity,\n",
    "                            'avg_precursor_rt' : avg_precursor_rt,\n",
    "                            'avg_precursor_volume' : avg_precursor_volume,\n",
    "                            'avg_normalized_area' : avg_normalized_area,\n",
    "                            'charge_state' : charge_state,\n",
    "                            'avg_ms2_cent_peaks' : avg_ms2_cent_peaks,\n",
    "                            'avg_raw_ms2_sample_peaks' :  avg_raw_ms2_sample_peaks,\n",
    "                            'mass_error_ppm' : float(mass_error_ppm),\n",
    "                            'rmse_mz' : float(rmse_mz),\n",
    "                            'unweighted_entropy_similarity' : unweighted_similarity,\n",
    "                            'entropy_similarity' : similarity,\n",
    "                            'dot_product': float(dot_product) if dot_product is not None else 'NA',\n",
    "                            'matches': []  # List to hold the top match\n",
    "                            }\n",
    "\n",
    "                        # Add the best match if available\n",
    "                        if best_match:\n",
    "                            linked_msp_feature['matches'].append(best_match)\n",
    "                        else:\n",
    "                            linked_msp_feature['matches'] = None  # No match found\n",
    "        \n",
    "                        linked_msp_features.append(linked_msp_feature)\n",
    "\n",
    "    # Return after all features are processed\n",
    "    return linked_msp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b16856",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_msp =link_msp(results, msp_data, mz_tolerance=0.025, rt_tolerance=8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739e057",
   "metadata": {},
   "source": [
    "10) Save your final results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e770ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  \n",
    "linked_msp_df = pd.DataFrame(linked_msp)\n",
    "filepath = Path('Annotations/linked_msp.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "linked_msp_df.to_csv(Path('Annotations/linked_msp.csv', index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e8b49",
   "metadata": {},
   "source": [
    "11) read_msp_file()\n",
    "Define the path to and read your MSP file. The function to read the MSP file will automatically convert the retention time from minutes to seconds, if your MSP data is already given in seconds you will have to remove that line of code. This code will also remove any annotations with missing or invalid retention times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7778f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse any strings that may be present in the MSP file\n",
    "def parse_array_from_string(s):\n",
    "    if isinstance(s, str):\n",
    "        return np.array([float(x) for x in re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", s)])\n",
    "    return np.array([])\n",
    "\n",
    "# Function to read and retrieve the annotations from an MSP file in the .msp format\n",
    "def read_msp_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    spectra_data = []\n",
    "    current_spectrum = {}\n",
    "    peak_data_started = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"Num Peaks\"):\n",
    "            peak_data_started = True\n",
    "            continue\n",
    "\n",
    "        if not peak_data_started:\n",
    "            if line.startswith(\"NAME:\"):\n",
    "                if \"|\" in line:\n",
    "                    # If the line contains '|', split the line after the '|' character\n",
    "                    parts = line.split('|')\n",
    "                    current_spectrum['name'] = parts[0].replace(\"NAME:\", \"\").strip()  # Remove \"NAME:\" and strip any extra spaces\n",
    "                    current_spectrum['saturation'] = parts[1].strip()  # After the '|'\n",
    "                else:\n",
    "                    # Otherwise, just use the name if no saturation is specified\n",
    "                    current_spectrum['name'] = line.split(\":\", 1)[1].strip()\n",
    "               \n",
    "            elif line.startswith(\"PRECURSORMZ:\"):\n",
    "                current_spectrum['precursor_mz'] = float(line.split(\":\", 1)[1].strip())\n",
    "            elif line.startswith(\"PRECURSORTYPE:\"):\n",
    "                current_spectrum['precursor_type'] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"IONMODE:\"):\n",
    "                current_spectrum['ion_mode'] = line.split(\":\", 1)[1].strip()              \n",
    "            elif line.startswith(\"RETENTIONTIME:\"):\n",
    "                _, raw = line.split(\":\", 1)\n",
    "                val = raw.strip()\n",
    "                if not val:\n",
    "                    print(\"[SKIP] empty retention_time\")\n",
    "                    continue\n",
    "                try:\n",
    "                    rt = float(val)\n",
    "                except ValueError:\n",
    "                        rt_parsed = parse_array_from_string(val)\n",
    "                        if isinstance(rt_parsed, (list, np.ndarray)) and len(rt_parsed) == 1:\n",
    "                            rt = float(rt_parsed[0])\n",
    "                        else:\n",
    "                            print(\"[SKIP] array invalid, skipping\")\n",
    "                            continue\n",
    "                current_spectrum['retention_time'] = rt * 60 # convert to seconds\n",
    "            elif line.startswith(\"Name: \"):\n",
    "                if current_spectrum:\n",
    "                    spectra_data.append(current_spectrum)\n",
    "                    current_spectrum = {\"Name\": line.split(\":\",1)[1].strip()}\n",
    "            elif line.startswith(\"FORMULA:\"):\n",
    "                current_spectrum['formula'] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"INCHIKEY:\"):\n",
    "                current_spectrum['inchi_key'] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"SMILES:\"):\n",
    "                current_spectrum['smiles'] = line.split(\":\", 1)[1].strip()\n",
    "            elif line.startswith(\"COMMENT:\"):\n",
    "                current_spectrum['comment'] = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                mz, intensity = map(float, line.split())\n",
    "                current_spectrum.setdefault('peaks', []).append((mz, intensity))\n",
    "            except ValueError:\n",
    "                # This is where the spectrum data is stored and new spectrum begins\n",
    "                if current_spectrum:\n",
    "                    spectra_data.append(current_spectrum)\n",
    "                current_spectrum = {}  # Reset for the next spectrum\n",
    "                peak_data_started = False  # Reset peak reading flag\n",
    "\n",
    "    # Add last spectrum if it exists\n",
    "    if current_spectrum:\n",
    "        spectra_data.append(current_spectrum)\n",
    "    \n",
    "    return spectra_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864cb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_file = r\"C:\\Users\\path_to_your_msp_file.msp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_data = read_msp_file(msp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba3f92",
   "metadata": {},
   "source": [
    "12) link_msp()\n",
    "Link the annotations from the MSP file to the dataframe containing all the information for your samples. The code was developed using an rt tolerance of 8 seconds and an m/z tolerance of 0.025. You can set the tolerance yourself, however, you may have to adjust the \"match_score\" calculations if you change the tolerance. This function only links annotations to features on the MS1 level, using the precursor rt and precursor m/z to determine a score that measures the weighted difference between the m/z and rt of the sample compared to the reference. This function performs quality checks on both the MS1 and MS2 level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cbb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import ms_entropy as me\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import ast\n",
    "\n",
    "# Function to link the annotations from the MSP file to the dataframe containing the MS1 and MS2 informataion for all of the samples provided.\n",
    "def link_msp(linked_features, msp_data, mz_tolerance=0.025, rt_tolerance=8.0):\n",
    "    linked_msp_features = []\n",
    "\n",
    "    # Loop through a list of Feature objects\n",
    "    for feature in linked_features:\n",
    "        cluster_id = feature['cluster_id']\n",
    "        file_id = feature['file_id']\n",
    "        feature_id = feature['feature_id']\n",
    "        feature_peak_ids = feature['feature_peak_ids']\n",
    "        peak_id = feature['peak_id']\n",
    "        msms_id = feature['msms_id']\n",
    "        precursor_mz = feature['precursor_mz']\n",
    "        precursor_rt = feature['precursor_rt']\n",
    "        precursor_intensity = feature['precursor_intensity']\n",
    "        precursor_volume = feature['precursor_vol']\n",
    "        average_ms1_mz = feature['average_ms1_mz']\n",
    "        average_rt = feature['average_rt']\n",
    "        charge_state = feature['charge_state']\n",
    "        ms2_sample_peaks = feature['ms2_sample_peaks']\n",
    "        total_intensity = feature['total_intensity']\n",
    "        total_volume = feature['total_volume']\n",
    "        normalized_area = precursor_volume * 114.7977026 # multiply the volume by the normalisation factor for comparison with MS-Dial\n",
    "\n",
    "        # Centroid the MS2 spectrum, using the default parameters but with normalize_intensity set to false.\n",
    "        centroided_peaks = me.clean_spectrum(\n",
    "            ms2_sample_peaks,\n",
    "            min_ms2_difference_in_da=0.02,\n",
    "            normalize_intensity=False\n",
    "        )\n",
    "\n",
    "        centroided_arr = np.array(centroided_peaks)\n",
    "        centroided_arr_list = centroided_arr.tolist()\n",
    "\n",
    "        # Find all matching annotations\n",
    "        best_match = None\n",
    "\n",
    "        for annotation in msp_data:\n",
    "            if 'precursor_mz' not in annotation or 'retention_time' not in annotation:\n",
    "                continue\n",
    "\n",
    "            # Calculate mz and rt distances, these are weighted to give a score between 0 and 1 and to prevent skewing from the retention time.\n",
    "            mz_distance = np.abs(precursor_mz - annotation['precursor_mz'])\n",
    "            rt_distance = np.abs(precursor_rt - annotation['retention_time'])\n",
    "\n",
    "            if mz_distance <= mz_tolerance and rt_distance <= rt_tolerance:\n",
    "                match_score = (rt_distance * 0.025) + (mz_distance * 20) # A lower score means a closer match between the sample and the reference in the MSP file.\n",
    "                mass_error_ppm = ((annotation['precursor_mz'] - precursor_mz) / annotation['precursor_mz']) * 1e6\n",
    "\n",
    "                y_true_mz = [annotation.get('precursor_mz')]\n",
    "                y_pred_mz = [precursor_mz]\n",
    "                rmse_mz = root_mean_squared_error(y_true_mz, y_pred_mz)\n",
    "\n",
    "                peaks_query = np.array(ms2_sample_peaks, dtype=np.float32)\n",
    "                peaks_reference = np.array([annotation.get('peaks')], dtype=np.float32)\n",
    "                if peaks_reference.ndim == 3 and peaks_reference.shape[0] == 1:\n",
    "                    peaks_reference = peaks_reference[0]  # flatten if needed\n",
    "\n",
    "                # Skip if still not 2D\n",
    "                if peaks_reference.ndim != 2:\n",
    "                    continue\n",
    "\n",
    "                unweighted_similarity = me.calculate_unweighted_entropy_similarity(peaks_query, peaks_reference)\n",
    "                similarity = me.calculate_entropy_similarity(peaks_query, peaks_reference)\n",
    "\n",
    "                reference_mz, reference_intensity = zip(*peaks_reference)\n",
    "                sample_mz, sample_intensity = zip(*centroided_peaks)\n",
    "\n",
    "                sample_mz = np.array(sample_mz)\n",
    "                sample_intensity = np.array(sample_intensity, dtype=np.float32)\n",
    "                reference_mz = np.array(reference_mz)\n",
    "                reference_intensity = np.array(reference_intensity, dtype=np.float32)\n",
    "\n",
    "                matched_indices = np.searchsorted(reference_mz, sample_mz)\n",
    "                matched_indices = matched_indices[matched_indices < len(reference_mz)]\n",
    "\n",
    "                sample_spectrum = sample_intensity[:len(matched_indices)]\n",
    "                reference_spectrum = reference_intensity[matched_indices]\n",
    "\n",
    "                sample_spectrum_normalized = sample_spectrum / np.linalg.norm(sample_spectrum)\n",
    "                reference_spectrum_normalized = reference_spectrum / np.linalg.norm(reference_spectrum)\n",
    "\n",
    "                dot_product = None\n",
    "                if len(sample_intensity) >= 3 and len(reference_intensity) >= 3:\n",
    "                    dot_product = np.dot(sample_spectrum_normalized, reference_spectrum_normalized)\n",
    "\n",
    "                if best_match is None or match_score < best_match['score']:\n",
    "                    best_match = {\n",
    "                        'score': float(match_score),\n",
    "                        'name': annotation.get('name', None),\n",
    "                        'saturation': annotation.get('saturation', None),\n",
    "                        'retention_time': annotation.get('retention_time', None),\n",
    "                        'precursor_mz': annotation.get('precursor_mz', None),\n",
    "                        'precursor_type': annotation.get('precursor_type', None),\n",
    "                        'smiles': annotation.get('smiles', None),\n",
    "                        'msp_peaks': annotation.get('peaks', None),\n",
    "                    }\n",
    "\n",
    "        linked_msp_feature = {\n",
    "            'cluster_id': cluster_id,\n",
    "            'file_id': file_id,\n",
    "            'feature_id': feature_id,\n",
    "            'feature_peak_ids': feature_peak_ids,\n",
    "            'peak_id': peak_id,\n",
    "            'msms_id': msms_id,\n",
    "            'average_ms1_mz': average_ms1_mz,\n",
    "            'total_intensity': total_intensity,\n",
    "            'average_rt': average_rt,\n",
    "            'total_volume': total_volume,\n",
    "            'precursor_mz': precursor_mz,\n",
    "            'precursor_intensity': precursor_intensity,\n",
    "            'precursor_rt': precursor_rt,\n",
    "            'precursor_volume': precursor_volume,\n",
    "            'normalized_area': normalized_area,\n",
    "            'charge_state': charge_state,\n",
    "            'centroided_ms2_peaks': centroided_arr_list,\n",
    "            'mass_error_ppm': float(mass_error_ppm) if best_match else None,\n",
    "            'rmse_mz': float(rmse_mz) if best_match else None,\n",
    "            'unweighted_entropy_similarity': unweighted_similarity if best_match else None,\n",
    "            'entropy_similarity': similarity if best_match else None,\n",
    "            'dot_product': float(dot_product) if dot_product is not None else 'NA',\n",
    "            'matches': [best_match] if best_match else None,\n",
    "            'raw_ms2_peaks' : ms2_sample_peaks,\n",
    "            'msp_peaks' : best_match['msp_peaks'] if best_match else None\n",
    "        }\n",
    "\n",
    "        linked_msp_features.append(linked_msp_feature)\n",
    "\n",
    "    return linked_msp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6589a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_msp = link_msp(linked_features, msp_data, mz_tolerance=0.025, rt_tolerance=8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da325d1d",
   "metadata": {},
   "source": [
    "13) Save your final results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86089f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  \n",
    "linked_msp_df = pd.DataFrame(linked_msp)\n",
    "filepath = Path('Annotations/linked_msp.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "linked_msp_df.to_csv(Path('Annotations/linked_msp.csv', index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PASTAQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
